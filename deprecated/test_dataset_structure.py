import os
import glob
import numpy as np
import scipy.io as sio

# --- é…ç½®åŒº ---
# è¯·å°†æ­¤è·¯å¾„ä¿®æ”¹ä¸ºæ‚¨æœåŠ¡å™¨ä¸Š sense-fi æ•°æ®é›†çš„å®é™…çˆ¶ç›®å½•
BASE_DATASET_DIR = '/home/cxy/data/code/datasets/sense-fi'
# è¾“å‡ºç›®å½•çš„åç§°
OUTPUT_DIR = 'sense-fi-samples'
# è¦ä»æ¯ä¸ªç±»åˆ«ä¸­æå–çš„æ ·æœ¬æ–‡ä»¶/è¡Œæ•°
NUM_SAMPLES_PER_CATEGORY = 3


def create_dir_if_not_exists(path):
    """åˆ›å»ºç›®å½•ï¼Œå¦‚æœç›®å½•å·²å­˜åœ¨åˆ™ä¸æ‰§è¡Œä»»ä½•æ“ä½œã€‚"""
    os.makedirs(path, exist_ok=True)


def generate_summary_file(output_dir):
    """ç”Ÿæˆæ•°æ®é›†æ ¼å¼çš„è¯¦ç»†åˆ†ææŠ¥å‘Šã€‚"""
    summary_text = """
# æ•°æ®é›†æ ¼å¼åˆ†ææŠ¥å‘Š

æœ¬æ–‡æ¡£æ—¨åœ¨è¯¦ç»†è¯´æ˜ `sense-fi` è·¯å¾„ä¸‹å››ä¸ªæ•°æ®é›†çš„æ–‡ä»¶æ ¼å¼ã€ç»“æ„ã€ç»´åº¦å’Œæ•°æ®å«ä¹‰ã€‚
æ‰€æœ‰åˆ†æå‡åŸºäºæä¾›çš„ç›®å½•æˆªå›¾å’Œ `dataset.txt` ä¸­çš„ Python å¤„ç†ä»£ç ã€‚

---

### æ•°æ®é›†ä¸€ï¼šWidardata

*   **æ–‡ä»¶è·¯å¾„ç»“æ„**: `Widardata/{train|test}/{activity_name}/{filename}.csv`
    *   ç¤ºä¾‹: `Widardata/train/22-Draw-10/user2-10-5-5-10-1-1e-07-100-20-100000-L0.csv`
*   **æ–‡ä»¶æ ¼å¼ä¸è¯»å–**:
    *   æ ¼å¼: CSV æ–‡ä»¶ (`.csv`)ã€‚
    *   è¯»å–æ–¹å¼: ä»£ç ä½¿ç”¨ `np.genfromtxt(..., delimiter=',')` è¯»å–ã€‚è¿™è¡¨æ˜æ–‡ä»¶æ˜¯é€—å·åˆ†éš”çš„çº¯æ–‡æœ¬ã€‚
*   **æ•°æ®ç»“æ„ä¸å«ä¹‰**:
    *   æ¯ä¸ª `.csv` æ–‡ä»¶ä»£è¡¨ä¸€ä¸ªç‹¬ç«‹çš„æ´»åŠ¨æ ·æœ¬ã€‚
    *   æ ¹æ®ä»£ç  `x.reshape(22, 20, 20)`ï¼Œæ¯ä¸ªæ–‡ä»¶åŒ…å« 8800 ä¸ªæ•°å€¼ï¼Œæœ€ç»ˆè¢«ç¨‹åºå¤„ç†æˆä¸€ä¸ª 22x20x20 çš„ä¸‰ç»´å¼ é‡ã€‚è¿™å¯èƒ½ä»£è¡¨ä»å¤šä¸ª Wi-Fi å­è½½æ³¢æ”¶é›†çš„CSIï¼ˆä¿¡é“çŠ¶æ€ä¿¡æ¯ï¼‰æ•°æ®ã€‚

---

### æ•°æ®é›†äºŒï¼šUT_HAR

*   **æ–‡ä»¶è·¯å¾„ç»“æ„**: `UT_HAR/{data|label}/{X|y}_{train|val|test}.csv`
*   **æ–‡ä»¶æ ¼å¼ä¸è¯»å–**:
    *   æ ¼å¼: å°½ç®¡æ‰©å±•åä¸º `.csv`ï¼Œä½†ä»£ç  `np.load(f)` è¡¨æ˜è¿™äº›æ–‡ä»¶å®é™…ä¸Šæ˜¯ NumPy çš„äºŒè¿›åˆ¶æ ¼å¼ (`.npy`)ã€‚
*   **æ•°æ®ç»“æ„ä¸å«ä¹‰**:
    *   **æ•°æ® (`X_*.csv`)**: æ–‡ä»¶åŒ…å«å¤šä¸ªæ ·æœ¬ã€‚`np.load()` ç›´æ¥åŠ è½½å‡ºä¸€ä¸ªä¸‰ç»´æ•°ç»„ï¼Œå½¢çŠ¶ç±»ä¼¼ `(N, 250, 90)`ï¼Œå…¶ä¸­ N æ˜¯æ ·æœ¬æ€»æ•°ã€‚æ¯ä¸ªæ ·æœ¬æ˜¯ä¸€ä¸ª `250x90` çš„äºŒç»´çŸ©é˜µï¼Œä»£è¡¨ä¸€ä¸ªæ´»åŠ¨å®ä¾‹çš„CSIæ•°æ®ã€‚
    *   **æ ‡ç­¾ (`y_*.csv`)**: è¿™æ˜¯ä¸€ä¸ªä¸€ç»´æ•°ç»„ï¼Œå…¶é•¿åº¦ä¸å¯¹åº”æ•°æ®æ–‡ä»¶çš„æ ·æœ¬æ•°ï¼ˆNï¼‰ç›¸åŒï¼Œæ¯ä¸ªå€¼æ˜¯å¯¹åº”æ•°æ®æ ·æœ¬çš„ç±»åˆ«æ ‡ç­¾ã€‚

---

### æ•°æ®é›†ä¸‰ï¼šNTU-Fi_HAR

*   **æ–‡ä»¶è·¯å¾„ç»“æ„**: `NTU-Fi_HAR/{train_amp|test_amp}/{activity_name}/{filename}.mat`
*   **æ–‡ä»¶æ ¼å¼ä¸è¯»å–**:
    *   æ ¼å¼: MATLAB æ•°æ®æ–‡ä»¶ (`.mat`)ã€‚
    *   è¯»å–æ–¹å¼: ä»£ç ä½¿ç”¨ `sio.loadmat(...)['CSIamp']` è¯»å–ï¼Œæå– `.mat` æ–‡ä»¶ä¸­åä¸º `CSIamp` çš„å˜é‡ã€‚
*   **æ•°æ®ç»“æ„ä¸å«ä¹‰**:
    *   æ¯ä¸ª `.mat` æ–‡ä»¶æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„æ´»åŠ¨æ ·æœ¬ã€‚
    *   æ ¹æ®ä»£ç çš„é‡‡æ ·å’Œé‡å¡‘æ“ä½œï¼Œ`CSIamp` å˜é‡åŸå§‹ç»´åº¦åº”ä¸ºä¸€ä¸ª 2D çŸ©é˜µï¼ˆæ¨æ–­ä¸º 342x2000ï¼‰ï¼Œä»£è¡¨ä¸€æ¬¡æ´»åŠ¨çš„CSIæŒ¯å¹…æ•°æ®ã€‚

---

### æ•°æ®é›†å››ï¼šNTU-Fi-HumanID

*   **æ–‡ä»¶è·¯å¾„ç»“æ„**: `NTU-Fi-HumanID/{train_amp|test_amp}/{person_id}/{filename}.mat`
*   **æ–‡ä»¶æ ¼å¼ä¸è¯»å–**: ä¸ `NTU-Fi_HAR` å®Œå…¨ç›¸åŒï¼Œè¯»å– `.mat` æ–‡ä»¶ä¸­çš„ `CSIamp` å˜é‡ã€‚
*   **æ•°æ®ç»“æ„ä¸å«ä¹‰**:
    *   ä¸ `NTU-Fi_HAR` ç»“æ„ç›¸åŒï¼Œæ¯ä¸ª `.mat` æ–‡ä»¶åŒ…å«ä¸€ä¸ªCSIæŒ¯å¹…çŸ©é˜µã€‚
    *   ä¸»è¦åŒºåˆ«åœ¨äºä»»åŠ¡ç›®æ ‡ï¼šè¿™é‡Œçš„æ ‡ç­¾æ˜¯äººçš„èº«ä»½IDï¼Œè€Œä¸æ˜¯æ´»åŠ¨ç±»åˆ«ã€‚
"""
    summary_filepath = os.path.join(output_dir, 'dataset_formats_summary.txt')
    with open(summary_filepath, 'w', encoding='utf-8') as f:
        f.write(summary_text.strip())
    print(f"âœ… å·²ç”Ÿæˆæ•°æ®é›†åˆ†ææŠ¥å‘Š: {summary_filepath}")


def process_widardata(base_dir, output_dir):
    """å¤„ç† Widardata æ•°æ®é›†ã€‚"""
    print("\n--- æ­£åœ¨å¤„ç† Widardata ---")
    dataset_path = os.path.join(base_dir, 'Widardata')
    for split in ['train', 'test']:
        split_path = os.path.join(dataset_path, split)
        if not os.path.isdir(split_path):
            continue

        category_dirs = sorted(glob.glob(os.path.join(split_path, '*/')))
        if not category_dirs:
            print(f"  - åœ¨ {split_path} ä¸­æœªæ‰¾åˆ°ç±»åˆ«æ–‡ä»¶å¤¹ï¼Œè·³è¿‡ã€‚")
            continue
        category_path = category_dirs[0]
        category_name = os.path.basename(os.path.normpath(category_path))

        sample_files = sorted(glob.glob(os.path.join(category_path, '*.csv')))[:NUM_SAMPLES_PER_CATEGORY]
        if not sample_files:
            print(f"  - åœ¨ {category_path} ä¸­æœªæ‰¾åˆ°.csvæ–‡ä»¶ï¼Œè·³è¿‡ã€‚")
            continue

        out_category_dir = os.path.join(output_dir, 'Widardata', split, category_name)
        create_dir_if_not_exists(out_category_dir)

        print(f"  - æ­£åœ¨ä» {category_path} æå–æ ·æœ¬...")
        for file_path in sample_files:
            try:
                data = np.genfromtxt(file_path, delimiter=',')
                filename = os.path.basename(file_path)
                output_filepath = os.path.join(out_category_dir, f"sample_{filename}")
                np.savetxt(output_filepath, data, delimiter=',', fmt='%f')
                print(f"    - å·²ä¿å­˜æ ·æœ¬åˆ°: {output_filepath}")
            except Exception as e:
                print(f"    - å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")


def process_ut_har(base_dir, output_dir):
    """å¤„ç† UT_HAR æ•°æ®é›†ã€‚"""
    print("\n--- æ­£åœ¨å¤„ç† UT_HAR ---")
    dataset_path = os.path.join(base_dir, 'UT_HAR')

    # å¤„ç†æ•°æ®æ–‡ä»¶ (X_*)
    data_path = os.path.join(dataset_path, 'data')
    out_data_path = os.path.join(output_dir, 'UT_HAR', 'data')
    create_dir_if_not_exists(out_data_path)
    print("  - æ­£åœ¨æå–æ•°æ® (X) æ ·æœ¬...")
    for split in ['train', 'test', 'val']:
        file_path = os.path.join(data_path, f'X_{split}.csv')
        if os.path.exists(file_path):
            try:
                # ä½¿ç”¨ np.load è¯»å–
                data_3d = np.load(file_path)

                # æå–å‰Nä¸ªæ ·æœ¬ (è¿™ä»ç„¶æ˜¯3Dçš„)
                samples_3d = data_3d[:NUM_SAMPLES_PER_CATEGORY]

                # *** å…³é”®ä¿®å¤ ***
                # å°†3Dæ ·æœ¬ (N, D1, D2) å‹å¹³ä¸º2D (N, D1*D2) ä»¥ä¾¿ä¿å­˜ä¸ºCSV
                num_samples = samples_3d.shape[0]
                samples_2d = samples_3d.reshape(num_samples, -1)

                output_filepath = os.path.join(out_data_path, f'sample_X_{split}.csv')
                np.savetxt(output_filepath, samples_2d, delimiter=',', fmt='%f')
                print(f"    - å·²ä¿å­˜æ ·æœ¬åˆ°: {output_filepath}")
            except Exception as e:
                print(f"    - å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")

    # å¤„ç†æ ‡ç­¾æ–‡ä»¶ (y_*)
    label_path = os.path.join(dataset_path, 'label')
    out_label_path = os.path.join(output_dir, 'UT_HAR', 'label')
    create_dir_if_not_exists(out_label_path)
    print("  - æ­£åœ¨æå–æ ‡ç­¾ (y) æ ·æœ¬...")
    for split in ['train', 'test', 'val']:
        file_path = os.path.join(label_path, f'y_{split}.csv')
        if os.path.exists(file_path):
            try:
                labels = np.load(file_path)
                samples = labels[:NUM_SAMPLES_PER_CATEGORY]

                output_filepath = os.path.join(out_label_path, f'sample_y_{split}.csv')
                np.savetxt(output_filepath, samples, delimiter=',', fmt='%d')
                print(f"    - å·²ä¿å­˜æ ·æœ¬åˆ°: {output_filepath}")
            except Exception as e:
                print(f"    - å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")


def _process_ntu_fi_generic(dataset_name, base_dir, output_dir):
    """å¤„ç† NTU-Fi_HAR å’Œ NTU-Fi-HumanID çš„é€šç”¨å‡½æ•°ã€‚"""
    print(f"\n--- æ­£åœ¨å¤„ç† {dataset_name} ---")
    dataset_path = os.path.join(base_dir, dataset_name)
    for split in ['train_amp', 'test_amp']:
        split_path = os.path.join(dataset_path, split)
        if not os.path.isdir(split_path):
            continue

        category_dirs = sorted(glob.glob(os.path.join(split_path, '*/')))
        if not category_dirs:
            print(f"  - åœ¨ {split_path} ä¸­æœªæ‰¾åˆ°ç±»åˆ«æ–‡ä»¶å¤¹ï¼Œè·³è¿‡ã€‚")
            continue
        category_path = category_dirs[0]
        category_name = os.path.basename(os.path.normpath(category_path))

        sample_files = sorted(glob.glob(os.path.join(category_path, '*.mat')))[:NUM_SAMPLES_PER_CATEGORY]
        if not sample_files:
            print(f"  - åœ¨ {category_path} ä¸­æœªæ‰¾åˆ°.matæ–‡ä»¶ï¼Œè·³è¿‡ã€‚")
            continue

        out_category_dir = os.path.join(output_dir, dataset_name, split, category_name)
        create_dir_if_not_exists(out_category_dir)

        print(f"  - æ­£åœ¨ä» {category_path} æå–æ ·æœ¬...")
        for file_path in sample_files:
            try:
                mat_contents = sio.loadmat(file_path)
                if 'CSIamp' not in mat_contents:
                    print(f"    - è­¦å‘Š: åœ¨ {file_path} ä¸­æœªæ‰¾åˆ° 'CSIamp' å˜é‡ï¼Œè·³è¿‡ã€‚")
                    continue
                data = mat_contents['CSIamp']

                filename = os.path.basename(file_path)
                csv_filename = os.path.splitext(filename)[0] + '.csv'
                output_filepath = os.path.join(out_category_dir, f"sample_{csv_filename}")

                np.savetxt(output_filepath, data, delimiter=',', fmt='%f')
                print(f"    - å·²ä¿å­˜æ ·æœ¬åˆ°: {output_filepath}")
            except Exception as e:
                print(f"    - å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")


def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    if not os.path.isdir(BASE_DATASET_DIR):
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ•°æ®é›†æ ¹ç›®å½• '{BASE_DATASET_DIR}'ã€‚è¯·æ£€æŸ¥å¹¶ä¿®æ”¹è„šæœ¬ä¸­çš„ BASE_DATASET_DIR å˜é‡ã€‚")
        return

    # 1. åˆ›å»ºä¸»è¾“å‡ºç›®å½•
    create_dir_if_not_exists(OUTPUT_DIR)
    print(f"è¾“å‡ºå°†ä¿å­˜åˆ°: {os.path.abspath(OUTPUT_DIR)}")

    # 2. ç”Ÿæˆåˆ†ææŠ¥å‘Š
    generate_summary_file(OUTPUT_DIR)

    # 3. å¤„ç†æ¯ä¸ªæ•°æ®é›†
    process_widardata(BASE_DATASET_DIR, OUTPUT_DIR)
    process_ut_har(BASE_DATASET_DIR, OUTPUT_DIR)
    _process_ntu_fi_generic('NTU-Fi_HAR', BASE_DATASET_DIR, OUTPUT_DIR)
    _process_ntu_fi_generic('NTU-Fi-HumanID', BASE_DATASET_DIR, OUTPUT_DIR)

    print("\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæ¯•ï¼")


if __name__ == "__main__":
    main()