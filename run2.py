#æœ¬æ–‡ä»¶ä¸ºä¸ºäº†æµ‹è¯•NMSEè€Œä¿®æ”¹çš„run.pyç‰ˆæœ¬ï¼Œå…¶ä»–éƒ¨åˆ†ä¸run.pyç›¸åŒ
import os  # å¼•å…¥ os æ¨¡å—
# ==================== è§£å†³ num_workers å’Œ numpy çš„å†²çª ====================
#ä¸ºäº†è®¾ç½®è¿›ç¨‹ä¸ºå•çº¿ç¨‹ï¼Œå‡å°‘cpuå ç”¨
# æ˜ç¡®æ§åˆ¶ OpenBLAS çš„çº¿ç¨‹æ•°ï¼Œè¿™æ˜¯ä½ å½“å‰NumPyçš„ä¸»è¦åç«¯
os.environ['OPENBLAS_NUM_THREADS'] = '1'
# OpenBLAS å†…éƒ¨ä½¿ç”¨ OpenMPï¼Œæ‰€ä»¥è¿™ä¸ªä¹Ÿå¾ˆé‡è¦
os.environ['OMP_NUM_THREADS'] = '1'
# ç”±äºä½ çš„numpyæ²¡æœ‰é“¾æ¥MKLï¼Œè¿™ä¸¤ä¸ªå˜é‡å¯ä»¥ä¸è®¾ï¼Œä½†è®¾äº†ä¹Ÿæ— å®³ï¼Œå¯ä»¥ä¿ç•™ä»¥é˜²ä¸‡ä¸€æœªæ¥æ›´æ”¹ç¯å¢ƒ
os.environ['MKL_NUM_THREADS'] = '1'
# è¿™ä¸ªé€šå¸¸æ˜¯macOSç‰¹æœ‰çš„ï¼ŒLinuxæœåŠ¡å™¨åŸºæœ¬ç”¨ä¸åˆ°ï¼Œä½†ä¿ç•™æ— å®³
os.environ['VECLIB_MAXIMUM_THREADS'] = '1'
# å¦‚æœä½ ç¡®å®šä¸ä½¿ç”¨BLISï¼Œè¿™ä¸ªå¯ä»¥ä¸è®¾ï¼Œä¿ç•™ä¹Ÿæ— å®³
os.environ['BLIS_NUM_THREADS'] = '1'
os.environ['NUMEXPR_NUM_THREADS'] = '1' # æœ‰æ—¶ä¹Ÿéœ€è¦è¿™ä¸ª
# =========================================================================
import numpy as np
import torch
#ä¸ºäº†è®¾ç½®è¿›ç¨‹ä¸ºå•çº¿ç¨‹ï¼Œå‡å°‘cpuå ç”¨
torch.set_num_threads(1)
import torch.nn as nn
import argparse
from util import load_data_n_model
import time
import csv # 1. å¼•å…¥ csv æ¨¡å—
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP


def nmse_miss_num_den(x_hat, x_gt, mask, eps=1e-12):
    """
    NMSE_miss = ||(1-M)âŠ™(x_hat-x_gt)||_F^2 / (||(1-M)âŠ™x_gt||_F^2 + eps)
    è¿™é‡Œ mask == M: è§‚æµ‹ç‚¹ä¸º1ï¼Œç¼ºå¤±ç‚¹ä¸º0
    è¿”å›ï¼šnum, den (torch.float64 æ ‡é‡å¼ é‡)
    """
    miss = (1.0 - mask)
    diff = (x_hat - x_gt) * miss
    num = (diff * diff).sum(dtype=torch.float64)
    den = ((x_gt * miss) * (x_gt * miss)).sum(dtype=torch.float64)
    return num, den + eps

def is_dist():
    return dist.is_available() and dist.is_initialized()

def get_rank():
    return dist.get_rank() if is_dist() else 0

def is_main():
    return get_rank() == 0



# train_one_epoch å’Œ test_one_epoch å‡½æ•°ä¸ä¸Šä¸€ä¸ªå›ç­”ä¸­çš„ç‰ˆæœ¬ç›¸åŒ
# è¿™é‡Œä¸ºäº†å®Œæ•´æ€§å†æ¬¡åŒ…å«å®ƒä»¬

'''def train_one_epoch(model, tensor_loader, criterion, device, optimizer):
    model.train()
    epoch_loss = 0
    epoch_accuracy = 0
    num_samples = 0

    for data in tensor_loader:
        inputs, labels = data
        inputs = inputs.to(device, dtype=torch.float32)
        labels = labels.to(device, dtype=torch.long)
        #labels = labels.type(torch.LongTensor)

        optimizer.zero_grad()
        outputs = model(inputs)
        #outputs = outputs.to(device)
        #outputs = outputs.type(torch.FloatTensor)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item() * inputs.size(0)
        predict_y = torch.argmax(outputs, dim=1)
        epoch_accuracy += (predict_y == labels).sum().item()
        num_samples += labels.size(0)

    epoch_loss = epoch_loss / num_samples
    epoch_accuracy = epoch_accuracy / num_samples
    return epoch_loss, epoch_accuracy'''

def train_one_epoch(
    model, tensor_loader, criterion, device, optimizer,
    is_rec: int = 0, criterion_rec=None, alpha: float = 0.5,
    grad_check=False           # æ˜¯å¦æ£€æŸ¥æ¢¯åº¦/å‚æ•°æ˜¯å¦åœ¨æ›´æ–°ï¼ˆdebug ç”¨ï¼‰
):
    model.train()
    epoch_loss = 0.0
    epoch_correct = 0
    num_samples = 0

    first_param_before = None
    if grad_check:
        # ç”¨æ¥éªŒè¯ optimizer çœŸçš„åœ¨æ›´æ–°å‚æ•°
        first_param_before = next(model.parameters()).detach().clone()

    for batch in tensor_loader:
        # ---- 1) æ¬åˆ° deviceï¼ˆä¿æŒ dtype æ­£ç¡®ï¼‰
        optimizer.zero_grad(set_to_none=True)
        if int(is_rec) == 0:
            inputs, labels = batch
            inputs = inputs.to(device, dtype=torch.float32, non_blocking=True)
            labels = labels.to(device, dtype=torch.long, non_blocking=True)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
        else:
            inputs, mask, labels, inputs_gt = batch
            inputs = inputs.to(device, dtype=torch.float32, non_blocking=True)
            mask = mask.to(device, dtype=torch.float32, non_blocking=True)
            labels = labels.to(device, dtype=torch.long, non_blocking=True)
            inputs_gt = inputs_gt.to(device, dtype=torch.float32, non_blocking=True)

            outputs, x_recon = model(inputs, mask)
            loss = criterion(outputs, labels) + float(alpha) * criterion_rec(x_recon, inputs_gt)

        loss.backward()
        optimizer.step()

        # ---- 4) ç»Ÿè®¡ epoch æŒ‡æ ‡
        bs = inputs.size(0)
        epoch_loss += loss.item() * bs
        pred = outputs.argmax(dim=1)
        epoch_correct += (pred == labels).sum().item()
        num_samples += bs


    if num_samples == 0:
        return 0.0, 0.0
    if is_dist():
        t = torch.tensor([epoch_loss, epoch_correct, num_samples],
                         device=device, dtype=torch.float64)
        dist.all_reduce(t, op=dist.ReduceOp.SUM)
        epoch_loss, epoch_correct, num_samples = t.tolist()
    epoch_loss = epoch_loss / num_samples
    epoch_accuracy = epoch_correct / num_samples

    if grad_check and first_param_before is not None:
        with torch.no_grad():
            first_param_after = next(model.parameters()).detach()
            delta = (first_param_after - first_param_before).abs().mean().item()
        if is_main():print(f"[grad_check] first_param abs-mean delta = {delta:.6e}")

    return epoch_loss, epoch_accuracy


def test_one_epoch(model, tensor_loader, criterion, device,
                   is_rec: int = 0, criterion_rec=None, alpha: float = 0.5,compute_nmse_miss: bool = True):
    model.eval()
    total_loss, total_correct, num_samples = 0.0, 0, 0
    nmse_num = torch.zeros(1, device=device, dtype=torch.float64)
    nmse_den = torch.zeros(1, device=device, dtype=torch.float64)

    with torch.no_grad():
        for batch in tensor_loader:
            if int(is_rec) == 0:
                inputs, labels = batch
                inputs = inputs.to(device, dtype=torch.float32, non_blocking=True)
                labels = labels.to(device, dtype=torch.long, non_blocking=True)

                outputs = model(inputs)
                loss = criterion(outputs, labels)

            else:
                inputs, mask, labels, inputs_gt = batch
                inputs = inputs.to(device, dtype=torch.float32, non_blocking=True)
                mask = mask.to(device, dtype=torch.float32, non_blocking=True)
                labels = labels.to(device, dtype=torch.long, non_blocking=True)
                inputs_gt = inputs_gt.to(device, dtype=torch.float32, non_blocking=True)

                outputs, x_recon = model(inputs, mask)
                if compute_nmse_miss:
                    num_i, den_i = nmse_miss_num_den(x_recon, inputs_gt, mask)
                    nmse_num += num_i
                    nmse_den += den_i

                loss = criterion(outputs, labels) + float(alpha) * criterion_rec(x_recon,  inputs_gt)

            bs = labels.size(0)
            total_loss += loss.item() * bs
            total_correct += (outputs.argmax(dim=1) == labels).sum().item()
            num_samples += bs
        if num_samples == 0:
            return 0.0, 0.0
        if is_dist():
            t = torch.tensor([total_loss, float(total_correct), float(num_samples)],
                             device=device, dtype=torch.float64)
            dist.all_reduce(t, op=dist.ReduceOp.SUM)
            total_loss, total_correct, num_samples = t.tolist()

            if compute_nmse_miss:
                dist.all_reduce(nmse_num, op=dist.ReduceOp.SUM)
                dist.all_reduce(nmse_den, op=dist.ReduceOp.SUM)

        test_loss = total_loss / num_samples
        test_acc  = total_correct / num_samples

        if compute_nmse_miss:
            nmse_miss = (nmse_num / nmse_den).item()
            return test_loss, test_acc, nmse_miss
        else:
            return test_loss, test_acc



def save_metrics_to_csv(filepath, history):
    """
    å°†æ€§èƒ½å†å²è®°å½•ï¼ˆä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼‰ä¿å­˜åˆ°CSVæ–‡ä»¶ã€‚
    Args:
        filepath (str): CSVæ–‡ä»¶çš„å®Œæ•´è·¯å¾„ã€‚
        history (list of dict): åŒ…å« 'epoch', 'loss', 'accuracy' çš„å­—å…¸åˆ—è¡¨ã€‚
    """
    if not history:
        return

    # ä½¿ç”¨ 'w' æ¨¡å¼æ‰“å¼€æ–‡ä»¶ï¼Œnewline='' æ˜¯csvæ¨¡å—çš„æ¨èåšæ³•
    with open(filepath, 'w', newline='') as f:
        # å®šä¹‰CSVæ–‡ä»¶çš„åˆ—åï¼Œä¸historyå­—å…¸ä¸­çš„é”®å¯¹åº”
        fieldnames = ['epoch', 'loss', 'accuracy']
        writer = csv.DictWriter(f, fieldnames=fieldnames)

        # å†™å…¥è¡¨å¤´
        writer.writeheader()
        # å†™å…¥æ‰€æœ‰è¡Œæ•°æ®
        writer.writerows(history)

def main():
    root = '../datasets/sense-fi/'
    if not os.path.isdir(root):
        if is_main():
            print(f"é”™è¯¯: æ•°æ®é›†æ ¹ç›®å½• '{root}' æœªæ‰¾åˆ°ã€‚")
            print("è¯·ç¡®è®¤æ‚¨çš„è„šæœ¬ï¼ˆrun.pyï¼‰æ˜¯å¦åœ¨ 'code/sense-fi/' æ–‡ä»¶å¤¹ä¸‹ï¼Œ")
            print("å¹¶ä¸” 'datasets' æ–‡ä»¶å¤¹åœ¨ 'code/' çš„ä¸Šä¸€çº§ç›®å½•ã€‚")
        return
    parser = argparse.ArgumentParser('WiFi Imaging Benchmark')
    parser.add_argument('--dataset', choices = ['UT_HAR_data','NTU-Fi-HumanID','NTU-Fi_HAR','Widar','Widar_digit_amp','Widar_digit_conj'])
    parser.add_argument('--model', choices = ['MLP','LeNet','ResNet18','ResNet50','ResNet101','RNN','GRU','LSTM','BiLSTM', 'CNN+GRU','ViT'])
    # æ–°å¢çš„å‚æ•°ï¼Œç”¨äºè‡ªå®šä¹‰å®éªŒåç§°ï¼Œå¹¶è®¾ä¸ºå¿…å¡«é¡¹
    #parser.add_argument('--exp_name', required=True, type=str, help='è‡ªå®šä¹‰å®éªŒåç§°ï¼Œå°†ç”¨äºåˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•ã€‚')
    parser.add_argument('--sample_rate', type=float, default=1.0, help='äºŒæ¬¡é™é‡‡æ ·çš„æ¯”ä¾‹ (0.05åˆ°1.0)ï¼Œå¯¹åº”25Hzåˆ°500Hzã€‚é»˜è®¤ä¸º1.0ï¼Œå³ä¸è¿›è¡ŒäºŒæ¬¡é‡‡æ ·ã€‚')
    parser.add_argument('--sample_method', type=str,default='uniform_nearest',choices=['uniform_nearest', 'equidistant', 'gaussian', 'poisson'],help='é™é‡‡æ ·æ–¹æ³•ã€‚é»˜è®¤ä¸º "uniform_nearest"ã€‚')
    parser.add_argument('--interpolation', type=str,default='linear',choices=['linear', 'cubic', 'nearest', 'idw', 'rbf','spline','akima'],help='å‡é‡‡æ ·æ—¶ä½¿ç”¨çš„æ’å€¼æ–¹æ³•ã€‚é»˜è®¤ä¸º "linear"ã€‚')
    parser.add_argument('--use_energy_input', type=int, default=1, choices=[0, 1],help='æ˜¯å¦ä½¿ç”¨èƒ½é‡ä¿¡æ¯ (1:æ˜¯, 0:å¦)ã€‚é»˜è®¤ä¸º 1 (æ˜¯)ã€‚')
    parser.add_argument('--use_mask_0', type=int, default=0, choices=[0, 1 , 2],help='æ˜¯å¦ä½¿ç”¨ mask_0 (1:æ˜¯, 0:å¦,2:ä¸maskç›´æ¥returné™é‡‡æ ·åçš„)ã€‚é»˜è®¤ä¸º 0 (å¦)ã€‚')
    # æ–°å¢ä¸¤ä¸ªå‚æ•°ï¼Œç”¨äºæ¥æ”¶å®Œæ•´çš„ä¿å­˜ç›®å½•
    parser.add_argument('--model_save_dir', required=True, type=str, help='æ¨¡å‹æ£€æŸ¥ç‚¹çš„å®Œæ•´ä¿å­˜ç›®å½•ã€‚')
    parser.add_argument('--metrics_save_dir', required=True, type=str, help='æ€§èƒ½æŒ‡æ ‡æ–‡ä»¶çš„å®Œæ•´ä¿å­˜ç›®å½•ã€‚')
    parser.add_argument('--is_rec', type=int, default=0, choices=[0, 1], help='1: é‡å»º+åˆ†ç±»ï¼›0: ä»…åˆ†ç±»')
    parser.add_argument('--rec_alpha', type=float, default=0.5, help='é‡å»ºæŸå¤±æƒé‡')
    parser.add_argument('--csdc_blocks', type=int, default=1, help='é‡å»ºblocksæ•°é‡')
    parser.add_argument('--rec_model', type=str, default='csdc', choices=['csdc', 'istanet'], help='é‡å»ºæ¨¡å‹ç±»å‹')
    parser.add_argument('--global_batch_size', type=int, default=128, help='å…¨å±€batch(æ‰€æœ‰GPUåŠ èµ·æ¥)')
    parser.add_argument('--num_workers_train', type=int, default=6)
    parser.add_argument('--num_workers_test', type=int, default=2)
    parser.add_argument('--eval_only', action='store_true', help='åªè·‘æµ‹è¯•ï¼Œä¸è®­ç»ƒ')
    parser.add_argument('--ckpt_path', type=str, default='', help='è¦åŠ è½½çš„æ¨¡å‹æƒé‡è·¯å¾„ï¼ˆé»˜è®¤ç”¨ model_save_dir/best_model.pthï¼‰')

    args = parser.parse_args()
    # ---- DDP init (torchrun ä¼šè®¾ç½®è¿™äº›ç¯å¢ƒå˜é‡) ----
    ddp = ("RANK" in os.environ) and ("WORLD_SIZE" in os.environ)
    if ddp:
        dist.init_process_group(backend="nccl", init_method="env://")
        local_rank = int(os.environ["LOCAL_RANK"])
        torch.cuda.set_device(local_rank)
        device = torch.device("cuda", local_rank)
        world_size = dist.get_world_size()
        rank = dist.get_rank()
    else:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        world_size = 1
        rank = 0

    # ---- å…¨å±€128 => æ¯å¡ batch = 128/world_size ----
    if args.global_batch_size % world_size != 0:
        raise ValueError(f"global_batch_size={args.global_batch_size} ä¸èƒ½è¢« world_size={world_size} æ•´é™¤")
    per_gpu_bs = args.global_batch_size // world_size
    train_loader, test_loader, model, train_epoch = load_data_n_model(
        args.dataset, args.model, root,
        args.sample_rate, args.sample_method, args.interpolation,
        args.use_energy_input, args.use_mask_0,
        args.is_rec, args.csdc_blocks, args.rec_model,
        batch_size=per_gpu_bs,
        num_workers_train=args.num_workers_train,
        num_workers_test=args.num_workers_test,
        distributed=ddp, rank=rank, world_size=world_size
    )

    #train_loader, test_loader, model, train_epoch = load_data_n_model(args.dataset, args.model, root,args.sample_rate, args.sample_method ,args.interpolation,args.use_energy_input ,args.use_mask_0 ,args.is_rec,args.csdc_blocks)
    criterion = nn.CrossEntropyLoss()
    criterion_rec = nn.MSELoss(reduction='mean') if args.is_rec else None

    #device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.to(device)
    if ddp:
        model = DDP(model, device_ids=[local_rank], output_device=local_rank)

    # ===== eval only: load checkpoint and run test =====
    if args.eval_only:
        ckpt_path = args.ckpt_path.strip()
        if ckpt_path == '':
            ckpt_path = os.path.join(args.model_save_dir, 'best_model.pth')
        if is_main():
            print(f"ğŸ§ª Eval-only mode. Loading ckpt from: {ckpt_path}")

        state = torch.load(ckpt_path, map_location='cpu')
        if ddp:
            model.module.load_state_dict(state, strict=True)
        else:
            model.load_state_dict(state, strict=True)

        # åªåœ¨ is_rec=1 ä¸” dataloader æä¾› (inputs, mask, labels, inputs_gt) æ—¶æ‰æœ‰æ„ä¹‰
        out = test_one_epoch(model, test_loader, criterion, device,
                             is_rec=args.is_rec, criterion_rec=criterion_rec, alpha=args.rec_alpha,
                             compute_nmse_miss=True)

        if len(out) == 3:
            test_loss, test_acc, nmse_miss = out
            if is_main():
                print(f"[Eval] Loss={test_loss:.5f}, Acc={test_acc:.4f}, NMSE_miss={nmse_miss:.6e}")
        else:
            test_loss, test_acc = out
            if is_main():
                print(f"[Eval] Loss={test_loss:.5f}, Acc={test_acc:.4f}")
                print("âš ï¸ NMSE_miss æœªè®¡ç®—ï¼šéœ€è¦ --is_rec=1 ä¸” test batch å« mask å’Œ gtã€‚")

        if ddp:
            dist.destroy_process_group()
        return

    # ===== training mode =====
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)



    # --- ç›®å½•åˆ›å»º ---
    # ç°åœ¨ run.py åªè´Ÿè´£ç¡®ä¿ç›®å½•å­˜åœ¨ï¼Œä¸å†æ„å»ºå®ƒ
    os.makedirs(args.model_save_dir, exist_ok=True)
    os.makedirs(args.metrics_save_dir, exist_ok=True)
    if is_main():
        print(f"âœ… æ¨¡å‹å°†ä¿å­˜è‡³: {os.path.abspath(args.model_save_dir)}")
        print(f"ğŸ“Š æ€§èƒ½æŒ‡æ ‡å°†ä¿å­˜è‡³: {os.path.abspath(args.metrics_save_dir)}")
    # ================================================================
    # 2. è®¡ç®—ä¿å­˜é—´éš”å’Œä¿å­˜ç‚¹
    num_saves = 4
    if train_epoch < num_saves:
        # å¦‚æœæ€»epochæ•°å°äº10ï¼Œåˆ™æ¯ä¸ªepochéƒ½ä¿å­˜
        save_interval = 1
    else:
        save_interval = train_epoch // num_saves

    # åˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰éœ€è¦ä¿å­˜çš„epochç¼–å·çš„é›†åˆï¼Œæ–¹ä¾¿å¿«é€ŸæŸ¥æ‰¾
    save_epochs = set(range(save_interval, train_epoch + 1, save_interval))
    # ç¡®ä¿æœ€åä¸€ä¸ªepochæ€»æ˜¯è¢«ä¿å­˜
    save_epochs.add(train_epoch)
    if is_main():print(f"æ¨¡å‹å°†ä¼šåœ¨ä»¥ä¸‹Epochç»“æŸæ—¶ä¿å­˜: {sorted(list(save_epochs))}")
    # ==========================================================

    # ==================== 4. æ–°å¢ï¼šåˆå§‹åŒ–å†å²è®°å½•åˆ—è¡¨ ====================
    train_history = []
    test_history = []

    # [æ–°å¢] æ—©åœç›¸å…³çš„å˜é‡
    best_test_acc = 0.0  # è®°å½•å†å²æœ€ä½³å‡†ç¡®ç‡
    patience = 20  # å®¹å¿åº¦ï¼šå¦‚æœ 20 ä¸ª epoch æ²¡æå‡å°±åœæ­¢
    patience_counter = 0  # è®¡æ•°å™¨
    # ===================================================
    # --- è®­ç»ƒä¸»å¾ªç¯ ---
    total_train_start = time.time()
    for epoch in range(1, train_epoch + 1):  # å¾ªç¯ä»1å¼€å§‹ï¼Œæ–¹ä¾¿ä¸epochç¼–å·å¯¹åº”
        if ddp and hasattr(train_loader.sampler, "set_epoch"):
            train_loader.sampler.set_epoch(epoch)
        if is_main():print(f"--- Epoch {epoch}/{train_epoch} ---")

        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, device, optimizer,
                                                is_rec=args.is_rec, criterion_rec=criterion_rec, alpha=args.rec_alpha)
        if is_main():print(f"Train -> Loss: {train_loss:.5f}, Accuracy: {train_acc:.4f}")

        test_loss, test_acc = test_one_epoch(model, test_loader, criterion, device,
                                             is_rec=args.is_rec, criterion_rec=criterion_rec, alpha=args.rec_alpha)
        if is_main():print(f"Test/Validation -> Loss: {test_loss:.5f}, Accuracy: {test_acc:.4f}")

        # ==================== 5. æ–°å¢ï¼šæ”¶é›†å½“å‰epochçš„æ•°æ® ====================
        train_history.append({'epoch': epoch, 'loss': train_loss, 'accuracy': train_acc})
        test_history.append({'epoch': epoch, 'loss': test_loss, 'accuracy': test_acc})

        # ==================== [æ ¸å¿ƒä¿®æ”¹] æ—©åœä¸æœ€ä½³æ¨¡å‹ä¿å­˜ ====================
        if test_acc > best_test_acc:
            best_test_acc = test_acc
            patience_counter = 0  # é‡ç½®è®¡æ•°å™¨

            # ä¿å­˜æœ€ä½³æ¨¡å‹ (è¦†ç›–å¼ä¿å­˜ï¼Œå§‹ç»ˆåªæœ‰ä¸€ä¸ª best_model.pth)
            # åªæœ‰å½“ Epoch > 10 ä»¥åï¼Œæ‰çœŸæ­£å¼€å§‹æ‰§è¡Œä¿å­˜ç¡¬ç›˜çš„æ“ä½œ
            if epoch > 20:
                best_model_path = os.path.join(args.model_save_dir, 'best_model.pth')
                if is_main():
                    state = model.module.state_dict() if ddp else model.state_dict()
                    torch.save(state, best_model_path)
                    print(f"ğŸŒŸ æ–°çºªå½•ï¼æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (Acc: {best_test_acc:.4f})")
            else:
                if is_main():print(f"ğŸŒŸ æ–°çºªå½• (Acc: {best_test_acc:.4f}) - è®­ç»ƒåˆæœŸæš‚ä¸ä¿å­˜")

        else:
            # åŒæ ·ï¼Œå‰ 10 ä¸ª Epoch ä¹Ÿä¸æ¶ˆè€— patienceï¼ˆå®½å®¹æœŸï¼‰
            if epoch > 20:
                patience_counter += 1
                if is_main():print(f"âš ï¸ æ€§èƒ½æœªæå‡ ({patience_counter}/{patience})")

        # æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢
        if patience_counter >= patience:
            if is_main():
                print(f"\nğŸ›‘ è§¦å‘æ—©åœæœºåˆ¶ï¼æµ‹è¯•é›†å‡†ç¡®ç‡å·²è¿ç»­ {patience} ä¸ª Epoch æœªæå‡ã€‚")
                print(f"   å½“å‰æœ€ä½³å‡†ç¡®ç‡: {best_test_acc:.4f}")
                print(f"   åœ¨ Epoch {epoch} åœæ­¢è®­ç»ƒã€‚")
            break  # è·³å‡º for å¾ªç¯
        # ===================================================================

        # --- æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ä¿å­˜ç‚¹ ---#å‰é¢æœ‰ä¿å­˜æœ€ä½³æ¨¡å‹äº†ï¼Œæ‰€ä»¥è¿™é‡Œä¸å†ä¿å­˜ã€‚
        '''if epoch in save_epochs:
            model_save_path = os.path.join(args.model_save_dir, f'model_epoch_{epoch}.pth')
            print(f"ğŸ’¾ åˆ°è¾¾ä¿å­˜ç‚¹ï¼Œæ­£åœ¨ä¿å­˜æ¨¡å‹åˆ°: {model_save_path}")
            torch.save(model.state_dict(), model_save_path)'''

    total_train_end = time.time()
    if is_main():
        print("\n--- è®­ç»ƒå®Œæˆ ---")
        print(f"â±ï¸ æ€»è®­ç»ƒè€—æ—¶ï¼š{total_train_end - total_train_start:.2f} ç§’")

    # ä½¿ç”¨æ–°çš„ç›®å½•å‚æ•°æ¥æ„å»ºè·¯å¾„
    train_metrics_path = os.path.join(args.metrics_save_dir, 'train_metrics.csv')
    test_metrics_path = os.path.join(args.metrics_save_dir, 'test_metrics.csv')

    if is_main():
        print(f"ğŸ“Š æ­£åœ¨ä¿å­˜è®­ç»ƒå†å²åˆ°: {train_metrics_path}")
        save_metrics_to_csv(train_metrics_path, train_history)

    if is_main():
        print(f"ğŸ“Š æ­£åœ¨ä¿å­˜æµ‹è¯•å†å²åˆ°: {test_metrics_path}")
        save_metrics_to_csv(test_metrics_path, test_history)

    #print(f"ğŸ’¾ æ‰€æœ‰æ£€æŸ¥ç‚¹å·²ä¿å­˜åœ¨ç›®å½•: {args.model_save_dir}")
    if ddp:
        dist.destroy_process_group()

if __name__ == "__main__":
    main()
