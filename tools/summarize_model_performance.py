#è¿è¡Œæ–¹æ³•ï¼š
# python summarize_model_performance_combined.py --exp_name "amp_500hz_baseline_20250724_2241"
#å°†ç°æœ‰çš„ compare_experiments.py è„šæœ¬è¿›è¡Œç®€åŒ–å’Œæ”¹é€ ï¼Œä½¿å…¶åŠŸèƒ½å˜ä¸ºï¼šåªåˆ†æä¸€ä¸ªæŒ‡å®šçš„å®éªŒï¼ˆä¾‹å¦‚ amp_500hz_baseline_20250724_2241ï¼‰ï¼Œ
# å¹¶ä¸ºè¿™ä¸ªå®éªŒä¸­çš„æ‰€æœ‰ä¸åŒæ¨¡å‹ç”Ÿæˆä¸€ä¸ªæ€§èƒ½å¯¹æ¯”çš„æ€»ç»“å›¾è¡¨å’Œç»Ÿè®¡æ•°æ®ã€‚
'''è„šæœ¬è¿è¡Œåï¼Œä¼šåœ¨ datasets/sense-fi/NTU-Fi_HAR/Analysis/ ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªæ–°çš„æ–‡ä»¶å¤¹ï¼Œä¾‹å¦‚ amp_500hz_baseline_..._summaryã€‚å…¶ä¸­åŒ…å«ï¼š
ç»ˆç«¯è¾“å‡ºå’ŒCSVæ–‡ä»¶ (model_performance_summary.csv):
ä¸€ä¸ªæ¸…æ™°çš„æ’è¡Œæ¦œï¼ŒæŒ‰â€œæœ€ä½³å‡†ç¡®ç‡â€ä»é«˜åˆ°ä½æ’åˆ—äº†æ‰€æœ‰11ä¸ªæ¨¡å‹ã€‚
æ¯ä¸€è¡Œéƒ½æ˜¾ç¤ºäº†æ¨¡å‹åç§°ã€å®ƒè¾¾åˆ°çš„æœ€ä½³å‡†ç¡®ç‡ä»¥åŠæ˜¯åœ¨å“ªä¸ªepochè¾¾åˆ°çš„ã€‚
ç¤ºä¾‹è¾“å‡º:
Generated code
============================================================
               æ¨¡å‹æ€§èƒ½æ’è¡Œæ¦œ: amp_500hz_baseline_...
============================================================
       Model  Best Accuracy  Best Epoch
0   ResNet18         0.9895          28
1    CNN+GRU         0.9870          25
2        ViT         0.9850          29
...
10       RNN         0.9420          30
============================================================
Use code with caution.
æ€§èƒ½å¯¹æ¯”æ¡å½¢å›¾ (model_performance_barchart.png):
ä¸€å¼ éå¸¸ç›´è§‚çš„æ¡å½¢å›¾ã€‚
Yè½´æ˜¯æ¨¡å‹åç§°ï¼ŒXè½´æ˜¯æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡ã€‚
æ¡å½¢æŒ‰æ€§èƒ½ä»é«˜åˆ°ä½æ’åˆ—ã€‚
æ¯ä¸ªæ¡å½¢æ—è¾¹éƒ½æ ‡æ³¨äº†ç²¾ç¡®çš„å‡†ç¡®ç‡æ•°å€¼ã€‚
è¿™å¼ å›¾éå¸¸é€‚åˆç›´æ¥ç”¨åœ¨æ‚¨çš„æŠ¥å‘Šæˆ–PPTä¸­ï¼Œç”¨æ¥å±•ç¤ºå“ªä¸ªæ¨¡å‹æ¶æ„åœ¨è¿™æ¬¡å®éªŒä¸­è¡¨ç°æœ€å¥½ã€‚'''

'''import os
import pandas as pd
import matplotlib.pyplot as plt
import argparse
import seaborn as sns

# --- é…ç½®åŒº ---
MODELS_TO_ANALYZE = [
    'MLP', 'LeNet', 'ResNet18', 'ResNet50', 'ResNet101', 'RNN',
    'GRU', 'LSTM', 'BiLSTM', 'CNN+GRU', 'ViT'
]


def analyze_single_experiment(base_path, dataset_name, exp_name):
    """ä¸»åˆ†æå‡½æ•°ï¼ŒåŠ è½½å•ä¸ªå®éªŒä¸‹æ‰€æœ‰æ¨¡å‹çš„æ•°æ®ï¼Œå¹¶ç”Ÿæˆæ€»ç»“ã€‚"""

    # å®šä¹‰è¾“å‡ºç›®å½•
    output_dir = os.path.join(base_path, dataset_name, "Analysis", exp_name + "_summary")
    os.makedirs(output_dir, exist_ok=True)
    print(f"ğŸ“Š åˆ†æç»“æœå°†ä¿å­˜è‡³: {os.path.abspath(output_dir)}")

    model_performance_data = []

    # 1. éå†æ‰€æœ‰æ¨¡å‹ï¼Œæ”¶é›†æ€§èƒ½æ•°æ®
    for model_name in MODELS_TO_ANALYZE:
        print(f"\n--- æ­£åœ¨å¤„ç†æ¨¡å‹: {model_name} ---")
        try:
            # æ„å»ºæŒ‡æ ‡æ–‡ä»¶è·¯å¾„
            metrics_path = os.path.join(base_path, dataset_name, "Metrics", exp_name, model_name, "test_metrics.csv")

            if not os.path.exists(metrics_path):
                print(f"  - è­¦å‘Š: æ‰¾ä¸åˆ°æŒ‡æ ‡æ–‡ä»¶ {metrics_path}ï¼Œè·³è¿‡ã€‚")
                continue

            df_test = pd.read_csv(metrics_path)
            if df_test.empty:
                print(f"  - è­¦å‘Š: æŒ‡æ ‡æ–‡ä»¶ä¸ºç©ºï¼Œè·³è¿‡ã€‚")
                continue

            # æå–å…³é”®æŒ‡æ ‡
            best_acc = df_test['accuracy'].max()
            best_epoch = df_test['accuracy'].idxmax() + 1

            model_performance_data.append({
                "Model": model_name,
                "Best Accuracy": best_acc,
                "Best Epoch": best_epoch
            })
            print(f"  - æ‰¾åˆ°æœ€ä½³å‡†ç¡®ç‡: {best_acc:.4f} (åœ¨ Epoch {best_epoch})")

        except Exception as e:
            print(f"  - é”™è¯¯: å¤„ç†æ¨¡å‹ '{model_name}' æ—¶å‡ºé”™: {e}")

    if not model_performance_data:
        print("\næœªèƒ½æ”¶é›†åˆ°ä»»ä½•æœ‰æ•ˆçš„æ¨¡å‹æ€§èƒ½æ•°æ®ã€‚")
        return

    # å°†ç»“æœè½¬æ¢ä¸º DataFrame å¹¶æ’åº
    summary_df = pd.DataFrame(model_performance_data)
    summary_df = summary_df.sort_values(by="Best Accuracy", ascending=False).reset_index(drop=True)

    # 2. æ‰“å°æ€»ç»“è¡¨æ ¼åˆ°ç»ˆç«¯
    print("\n\n" + "=" * 60)
    print(" " * 15 + f"æ¨¡å‹æ€§èƒ½æ’è¡Œæ¦œ: {exp_name}")
    print("=" * 60)
    print(summary_df.to_string())
    print("=" * 60)

    # 3. ä¿å­˜æ€»ç»“è¡¨æ ¼åˆ°CSV
    summary_csv_path = os.path.join(output_dir, "model_performance_summary.csv")
    summary_df.to_csv(summary_csv_path, index=False)
    print(f"âœ… è¯¦ç»†æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜è‡³: {summary_csv_path}")

    # 4. ç»˜åˆ¶å¹¶ä¿å­˜æ€§èƒ½å¯¹æ¯”æ¡å½¢å›¾
    plt.figure(figsize=(12, 8))
    barplot = sns.barplot(x="Best Accuracy", y="Model", data=summary_df, palette="viridis")

    # åœ¨æ¡å½¢å›¾ä¸Šæ˜¾ç¤ºæ•°å€¼
    for index, row in summary_df.iterrows():
        barplot.text(row["Best Accuracy"] + 0.001, index, f"{row['Best Accuracy']:.4f}",
                     color='black', ha="left", va='center')

    plt.title(f'Model Performance Comparison\nExperiment: {exp_name}', fontsize=16)
    plt.xlabel('Best Test Accuracy', fontsize=12)
    plt.ylabel('Model', fontsize=12)
    plt.xlim(summary_df['Best Accuracy'].min() * 0.98, summary_df['Best Accuracy'].max() * 1.02)  # è°ƒæ•´xè½´èŒƒå›´
    plt.grid(axis='x', linestyle='--', alpha=0.7)

    plot_path = os.path.join(output_dir, "model_performance_barchart.png")
    plt.savefig(plot_path, bbox_inches='tight')
    plt.close()
    print(f"ğŸ“ˆ æ€§èƒ½å¯¹æ¯”æ¡å½¢å›¾å·²ä¿å­˜è‡³: {plot_path}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Summarize and compare model performances for a single experiment.")
    parser.add_argument('--dataset_root', type=str, default='../../datasets/sense-fi/',
                        help='Path to the datasets root directory.')
    parser.add_argument('--dataset', type=str, default='NTU-Fi_HAR', help='Dataset name to analyze.')
    parser.add_argument('--exp_name', type=str, required=True, help='Name of the experiment to analyze.')

    args = parser.parse_args()

    analyze_single_experiment(args.dataset_root, args.dataset, args.exp_name)'''

#ä¸Šé¢æ˜¯æœ€ä¼˜ç»“æœï¼Œä¸‹é¢æ˜¯æœ€åepochç»“æœ
'''import os
import pandas as pd
import matplotlib.pyplot as plt
import argparse
import seaborn as sns

# --- é…ç½®åŒº ---
MODELS_TO_ANALYZE = [
    'MLP', 'LeNet', 'ResNet18', 'ResNet50', 'ResNet101', 'RNN',
    'GRU', 'LSTM', 'BiLSTM', 'CNN+GRU', 'ViT'
]


def analyze_single_experiment(base_path, dataset_name, exp_name):
    """ä¸»åˆ†æå‡½æ•°ï¼ŒåŠ è½½å•ä¸ªå®éªŒä¸‹æ‰€æœ‰æ¨¡å‹çš„æ•°æ®ï¼Œå¹¶ç”Ÿæˆæ€»ç»“ã€‚"""

    # å®šä¹‰è¾“å‡ºç›®å½•
    output_dir = os.path.join(base_path, dataset_name, "Analysis", exp_name + "_summary_final_epoch")
    os.makedirs(output_dir, exist_ok=True)
    print(f"ğŸ“Š åˆ†æç»“æœå°†ä¿å­˜è‡³: {os.path.abspath(output_dir)}")

    model_performance_data = []

    # 1. éå†æ‰€æœ‰æ¨¡å‹ï¼Œæ”¶é›†æ€§èƒ½æ•°æ®
    for model_name in MODELS_TO_ANALYZE:
        print(f"\n--- æ­£åœ¨å¤„ç†æ¨¡å‹: {model_name} ---")
        try:
            metrics_path = os.path.join(base_path, dataset_name, "Metrics", exp_name, model_name, "test_metrics.csv")

            if not os.path.exists(metrics_path):
                print(f"  - è­¦å‘Š: æ‰¾ä¸åˆ°æŒ‡æ ‡æ–‡ä»¶ {metrics_path}ï¼Œè·³è¿‡ã€‚")
                continue

            df_test = pd.read_csv(metrics_path)
            if df_test.empty:
                print(f"  - è­¦å‘Š: æŒ‡æ ‡æ–‡ä»¶ä¸ºç©ºï¼Œè·³è¿‡ã€‚")
                continue

            # ==================== æ ¸å¿ƒä¿®æ”¹åœ¨è¿™é‡Œ ====================
            # åŸæ¥çš„ä»£ç :
            # best_acc = df_test['accuracy'].max()
            # best_epoch = df_test['accuracy'].idxmax() + 1

            # æ–°çš„ä»£ç ï¼šæå–æœ€åä¸€ä¸ªepochçš„æ€§èƒ½
            if 'epoch' in df_test.columns and len(df_test) > 0:
                final_epoch_data = df_test.iloc[-1]
                final_acc = final_epoch_data['accuracy']
                final_epoch = final_epoch_data['epoch']
            else:
                print(f"  - è­¦å‘Š: {model_name} çš„CSVæ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®æˆ–ä¸ºç©ºï¼Œè·³è¿‡ã€‚")
                continue

            model_performance_data.append({
                "Model": model_name,
                "Final Accuracy": final_acc,  # åˆ—åæ”¹ä¸º Final Accuracy
                "Final Epoch": final_epoch  # åˆ—åæ”¹ä¸º Final Epoch
            })
            print(f"  - æ‰¾åˆ°æœ€åä¸€ä¸ª Epoch ({final_epoch}) çš„å‡†ç¡®ç‡: {final_acc:.4f}")
            # ========================================================

        except Exception as e:
            print(f"  - é”™è¯¯: å¤„ç†æ¨¡å‹ '{model_name}' æ—¶å‡ºé”™: {e}")

    if not model_performance_data:
        print("\næœªèƒ½æ”¶é›†åˆ°ä»»ä½•æœ‰æ•ˆçš„æ¨¡å‹æ€§èƒ½æ•°æ®ã€‚")
        return

    # å°†ç»“æœè½¬æ¢ä¸º DataFrame å¹¶æŒ‰æœ€ç»ˆå‡†ç¡®ç‡æ’åº
    summary_df = pd.DataFrame(model_performance_data)
    summary_df = summary_df.sort_values(by="Final Accuracy", ascending=False).reset_index(drop=True)

    # 2. æ‰“å°æ€»ç»“è¡¨æ ¼åˆ°ç»ˆç«¯
    print("\n\n" + "=" * 60)
    print(" " * 10 + f"æ¨¡å‹æœ€ç»ˆæ€§èƒ½æ’è¡Œæ¦œ (Final Epoch): {exp_name}")
    print("=" * 60)
    print(summary_df.to_string())
    print("=" * 60)

    # 3. ä¿å­˜æ€»ç»“è¡¨æ ¼åˆ°CSV
    summary_csv_path = os.path.join(output_dir, "model_performance_summary_final_epoch.csv")
    summary_df.to_csv(summary_csv_path, index=False)
    print(f"âœ… è¯¦ç»†æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜è‡³: {summary_csv_path}")

    # 4. ç»˜åˆ¶å¹¶ä¿å­˜æ€§èƒ½å¯¹æ¯”æ¡å½¢å›¾ (ç°åœ¨åŸºäº Final Accuracy)
    plt.figure(figsize=(12, 8))
    barplot = sns.barplot(x="Final Accuracy", y="Model", data=summary_df, palette="viridis_r")  # ä½¿ç”¨åè‰² viridis_r

    for index, row in summary_df.iterrows():
        barplot.text(row["Final Accuracy"] + 0.001, index, f"{row['Final Accuracy']:.4f}",
                     color='black', ha="left", va='center')

    plt.title(f'Model Performance Comparison (Final Epoch)\nExperiment: {exp_name}', fontsize=16)
    plt.xlabel('Final Test Accuracy', fontsize=12)
    plt.ylabel('Model', fontsize=12)
    min_acc = summary_df['Final Accuracy'].min()
    max_acc = summary_df['Final Accuracy'].max()
    plt.xlim(min_acc - (max_acc - min_acc) * 0.1, max_acc + (max_acc - min_acc) * 0.2)  # åŠ¨æ€è°ƒæ•´xè½´èŒƒå›´
    plt.grid(axis='x', linestyle='--', alpha=0.7)

    plot_path = os.path.join(output_dir, "model_performance_barchart_final_epoch.png")
    plt.savefig(plot_path, bbox_inches='tight')
    plt.close()
    print(f"ğŸ“ˆ æ€§èƒ½å¯¹æ¯”æ¡å½¢å›¾å·²ä¿å­˜è‡³: {plot_path}")


if __name__ == '__main__':
    # ... (argparse éƒ¨åˆ†ä¿æŒä¸å˜) ...
    parser = argparse.ArgumentParser(description="Summarize and compare model performances for a single experiment.")
    parser.add_argument('--dataset_root', type=str, default='../../datasets/sense-fi/',
                        help='Path to the datasets root directory.')
    parser.add_argument('--dataset', type=str, default='NTU-Fi_HAR', help='Dataset name to analyze.')
    parser.add_argument('--exp_name', type=str, required=True, help='Name of the experiment to analyze.')

    args = parser.parse_args()

    analyze_single_experiment(args.dataset_root, args.dataset, args.exp_name)'''

#ä¸‹é¢æ˜¯ä¸¤å¼ å›¾å¯¹æ¯”
'''å°†ä¸¤ç§å…³é”®æŒ‡æ ‡ï¼ˆæœ€ä½³æ€§èƒ½å’Œæœ€ç»ˆæ€§èƒ½ï¼‰æ”¾åœ¨åŒä¸€å¼ å›¾ä¸Šè¿›è¡Œå¯¹æ¯”ï¼Œå¯ä»¥éå¸¸ç›´è§‚åœ°æ­ç¤ºå‡ºæ¯ä¸ªæ¨¡å‹çš„è®­ç»ƒç¨³å®šæ€§å’Œè¿‡æ‹Ÿåˆæƒ…å†µã€‚
æˆ‘ä»¬å°†é‡‡ç”¨ä¸€ç§éå¸¸æ¸…æ™°çš„å›¾è¡¨â€”â€”å“‘é“ƒå›¾ï¼ˆDumbbell Plotï¼‰â€”â€”æ¥å®ç°è¿™ä¸ªæ•ˆæœã€‚å¯¹äºæ¯ä¸ªæ¨¡å‹ï¼Œå›¾ä¸Šä¼šæœ‰ä¸€æ¡æ°´å¹³çº¿ï¼Œçº¿çš„ä¸¤ç«¯åˆ†åˆ«æ˜¯å®ƒçš„â€œæœ€ä½³å‡†ç¡®ç‡â€å’Œâ€œæœ€ç»ˆå‡†ç¡®ç‡â€ã€‚
çº¿å¾ˆçŸ­ï¼šè¯´æ˜æ¨¡å‹æ”¶æ•›å¾—å¾ˆå¥½ï¼Œæœ€ç»ˆæ€§èƒ½æ¥è¿‘å…¶æ½œåŠ›å³°å€¼ã€‚
çº¿å¾ˆé•¿ï¼šè¯´æ˜æ¨¡å‹å¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆï¼Œåœ¨è®­ç»ƒåæœŸæ€§èƒ½æœ‰æ‰€ä¸‹é™ï¼Œæœ€ç»ˆæ€§èƒ½è¿œä½äºå…¶æ›¾è¾¾åˆ°è¿‡çš„æœ€ä½³æ°´å¹³ã€‚
æ•´ä¸ªå›¾è¡¨å’Œæ€»ç»“æŠ¥å‘Šå°†æŒ‰ç…§æ‚¨è¦æ±‚çš„**â€œæœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡ (Final Test Accuracy)â€**è¿›è¡Œæ’åºã€‚'''
import os
import pandas as pd
import matplotlib.pyplot as plt
import argparse
import seaborn as sns

# --- é…ç½®åŒº ---
MODELS_TO_ANALYZE = [
    'MLP', 'LeNet', 'ResNet18', 'ResNet50', 'ResNet101', 'RNN',
    'GRU', 'LSTM', 'BiLSTM', 'CNN+GRU', 'ViT'
]


def analyze_single_experiment_combined(base_path, dataset_name, exp_name):
    """
    ä¸»åˆ†æå‡½æ•°ï¼ŒåŠ è½½å•ä¸ªå®éªŒä¸‹æ‰€æœ‰æ¨¡å‹çš„æ•°æ®ï¼Œ
    æå–æœ€ä½³å’Œæœ€ç»ˆæ€§èƒ½ï¼Œå¹¶ç”Ÿæˆç»¼åˆå›¾è¡¨ä¸æŠ¥å‘Šã€‚
    """

    # å®šä¹‰è¾“å‡ºç›®å½•
    output_dir = os.path.join(base_path, dataset_name, "Analysis", exp_name + "_summary_combined")
    os.makedirs(output_dir, exist_ok=True)
    print(f"ğŸ“Š åˆ†æç»“æœå°†ä¿å­˜è‡³: {os.path.abspath(output_dir)}")

    model_performance_data = []

    # 1. éå†æ‰€æœ‰æ¨¡å‹ï¼Œæ”¶é›†ä¸¤ç§æ€§èƒ½æ•°æ®
    for model_name in MODELS_TO_ANALYZE:
        print(f"\n--- æ­£åœ¨å¤„ç†æ¨¡å‹: {model_name} ---")
        try:
            metrics_path = os.path.join(base_path, dataset_name, "Metrics", exp_name, model_name, "test_metrics.csv")

            if not os.path.exists(metrics_path):
                print(f"  - è­¦å‘Š: æ‰¾ä¸åˆ°æŒ‡æ ‡æ–‡ä»¶ {metrics_path}ï¼Œè·³è¿‡ã€‚")
                continue

            df_test = pd.read_csv(metrics_path)
            if df_test.empty:
                print(f"  - è­¦å‘Š: æŒ‡æ ‡æ–‡ä»¶ä¸ºç©ºï¼Œè·³è¿‡ã€‚")
                continue

            # æå–æœ€ä½³æ€§èƒ½
            best_acc = df_test['accuracy'].max()
            best_epoch = df_test['accuracy'].idxmax() + 1

            # æå–æœ€ç»ˆæ€§èƒ½
            final_acc = df_test.iloc[-1]['accuracy']
            final_epoch = df_test.iloc[-1]['epoch']

            model_performance_data.append({
                "Model": model_name,
                "Best Accuracy": best_acc,
                "Best Epoch": best_epoch,
                "Final Accuracy": final_acc,
                "Final Epoch": final_epoch
            })
            print(f"  - æœ€ç»ˆå‡†ç¡®ç‡: {final_acc:.4f} | æœ€ä½³å‡†ç¡®ç‡: {best_acc:.4f}")

        except Exception as e:
            print(f"  - é”™è¯¯: å¤„ç†æ¨¡å‹ '{model_name}' æ—¶å‡ºé”™: {e}")

    if not model_performance_data:
        print("\næœªèƒ½æ”¶é›†åˆ°ä»»ä½•æœ‰æ•ˆçš„æ¨¡å‹æ€§èƒ½æ•°æ®ã€‚")
        return

    # 2. å°†ç»“æœè½¬æ¢ä¸º DataFrame å¹¶æŒ‰ã€æœ€ç»ˆå‡†ç¡®ç‡ã€‘æ’åº
    summary_df = pd.DataFrame(model_performance_data)
    summary_df = summary_df.sort_values(by="Final Accuracy", ascending=False).reset_index(drop=True)

    # 3. æ‰“å°æ€»ç»“è¡¨æ ¼åˆ°ç»ˆç«¯
    print("\n\n" + "=" * 85)
    print(" " * 15 + f"æ¨¡å‹ç»¼åˆæ€§èƒ½æ’è¡Œæ¦œ (æŒ‰æœ€ç»ˆæ€§èƒ½æ’åº): {exp_name}")
    print("=" * 85)
    print(summary_df.to_string())
    print("=" * 85)

    # 4. ä¿å­˜æ€»ç»“è¡¨æ ¼åˆ°CSV
    summary_csv_path = os.path.join(output_dir, "model_performance_summary_combined.csv")
    summary_df.to_csv(summary_csv_path, index=False)
    print(f"âœ… è¯¦ç»†æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜è‡³: {summary_csv_path}")

    # 5. ç»˜åˆ¶å¹¶ä¿å­˜æ€§èƒ½å¯¹æ¯”å“‘é“ƒå›¾
    fig, ax = plt.subplots(figsize=(12, 8))

    # åè½¬yè½´ï¼Œè®©æ€§èƒ½æœ€å¥½çš„æ¨¡å‹åœ¨æœ€ä¸Šé¢
    ax.invert_yaxis()

    # ç»˜åˆ¶è¿æ¥çº¿
    ax.hlines(y=summary_df.index, xmin=summary_df['Final Accuracy'], xmax=summary_df['Best Accuracy'],
              color='grey', alpha=0.6, linestyle='--')

    # ç»˜åˆ¶æ•£ç‚¹
    ax.scatter(summary_df['Final Accuracy'], summary_df.index, color='dodgerblue', s=80,
               label='Final Accuracy', zorder=3)
    ax.scatter(summary_df['Best Accuracy'], summary_df.index, color='orangered', s=80,
               label='Best Accuracy', zorder=3)

    # è®¾ç½®Yè½´åˆ»åº¦ä¸ºæ¨¡å‹åç§°
    ax.set_yticks(summary_df.index)
    ax.set_yticklabels(summary_df['Model'])

    # æ·»åŠ æ ‡é¢˜å’Œæ ‡ç­¾
    ax.set_title(f'Model Performance: Best vs. Final Accuracy\nExperiment: {exp_name}', fontsize=16)
    ax.set_xlabel('Test Accuracy', fontsize=12)
    ax.set_ylabel('Model', fontsize=12)

    # è®¾ç½®å›¾ä¾‹
    ax.legend()

    # ä¼˜åŒ–å¸ƒå±€å’Œç½‘æ ¼
    min_val = summary_df[['Best Accuracy', 'Final Accuracy']].min().min()
    max_val = summary_df[['Best Accuracy', 'Final Accuracy']].max().max()
    ax.set_xlim(min_val - (max_val - min_val) * 0.05, max_val + (max_val - min_val) * 0.05)
    ax.grid(axis='x', linestyle='--', alpha=0.7)

    plot_path = os.path.join(output_dir, "model_performance_dumbbell_plot.png")
    plt.savefig(plot_path, bbox_inches='tight')
    plt.close()
    print(f"ğŸ“ˆ ç»¼åˆæ€§èƒ½å“‘é“ƒå›¾å·²ä¿å­˜è‡³: {plot_path}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description="Summarize and compare best vs. final model performances for a single experiment.")
    parser.add_argument('--dataset_root', type=str, default='../../datasets/sense-fi/',
                        help='Path to the datasets root directory.')
    parser.add_argument('--dataset', type=str, default='NTU-Fi_HAR', help='Dataset name to analyze.')
    parser.add_argument('--exp_name', type=str, required=True, help='Name of the experiment to analyze.')

    args = parser.parse_args()

    analyze_single_experiment_combined(args.dataset_root, args.dataset, args.exp_name)