#ä¸ºç½‘æ ¼æœç´¢ä¸­çš„æ¯ä¸€ä¸ªç‹¬ç«‹çš„å®éªŒï¼ˆå³æ¯ä¸€ä¸ª rate:method:model ç»„åˆï¼‰ï¼Œç”Ÿæˆå®ƒè‡ªå·±çš„è®­ç»ƒè¿‡ç¨‹å›¾å’Œä¸€ä»½è¯¦ç»†çš„æ€§èƒ½ç»Ÿè®¡æŠ¥å‘Šã€‚
#è¿è¡Œæ–¹æ³•ï¼š
#   python analyze_individual_runs.py --exp_name "energy_rate_interp_20250726_2329"
'''
    ç‹¬ç«‹çš„è®­ç»ƒæ›²çº¿å›¾ (training_curves.png):
    å¯¹äºæ¯ä¸€ä¸ªå®éªŒç»„åˆï¼Œéƒ½ä¼šç”Ÿæˆä¸€å¼ å›¾ã€‚
    è¿™å¼ å›¾æ¸…æ™°åœ°å±•ç¤ºäº†è¯¥è®¾ç½®ä¸‹ï¼Œæ¨¡å‹çš„train/testå‡†ç¡®ç‡å’ŒæŸå¤±éšepochçš„å˜åŒ–ã€‚
    å›¾ä¸­æ ‡å‡ºäº†æœ€ä½³å‡†ç¡®ç‡ï¼Œå¹¶ç”¨è™šçº¿æ ‡å‡ºäº†è¾¾åˆ°è¯¥å‡†ç¡®ç‡çš„ epochï¼Œæ–¹ä¾¿æ‚¨åˆ¤æ–­æ¨¡å‹æ˜¯å¦æ—©åœæˆ–è¿‡æ‹Ÿåˆã€‚
    ç»Ÿè®¡æ€»è§ˆCSV (all_runs_statistics.csv):
    è¿™æ˜¯ä¸€ä¸ªå·¨å¤§çš„è¡¨æ ¼ï¼Œæœ‰605è¡Œï¼ˆæˆ–æ›´å¤šï¼Œå–å†³äºæ‚¨å®éªŒçš„å®Œæ•´åº¦ï¼‰ã€‚
    æ¯ä¸€è¡Œéƒ½å¯¹åº”ä¸€æ¬¡ç‹¬ç«‹çš„è®­ç»ƒï¼Œè®°å½•äº†å®ƒçš„æ‰€æœ‰å‚æ•°ï¼ˆmodel, rate, methodï¼‰å’Œå…³é”®çš„æ€§èƒ½æŒ‡æ ‡ï¼ˆæœ€ä½³å‡†ç¡®ç‡ã€æœ€ç»ˆå‡†ç¡®ç‡ã€è¿‡æ‹Ÿåˆå·®è·ç­‰ï¼‰ã€‚
    è¿™ä¸ªæ–‡ä»¶æ˜¯è¿›è¡Œåç»­æ•°æ®åˆ†æçš„å®è´µè´¢å¯Œã€‚æ‚¨å¯ä»¥å°†å®ƒå¯¼å…¥Excelæˆ–Jupyter Notebookï¼Œè¿›è¡Œæ’åºã€ç­›é€‰ï¼Œæ‰¾åˆ°å„ç§æœ‰è¶£çš„è§„å¾‹ã€‚'''
import os
import pandas as pd
import matplotlib.pyplot as plt
import argparse

# --- é…ç½®åŒº (ä¸ä¹‹å‰çš„åˆ†æè„šæœ¬ä¿æŒä¸€è‡´) ---
MODELS = [
    'MLP', 'LeNet', 'ResNet18', 'ResNet50', 'ResNet101', 'RNN',
    'GRU', 'LSTM', 'BiLSTM', 'CNN+GRU', 'ViT'
]
SAMPLE_RATES = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
INTERPOLATION_METHODS = ['linear', 'cubic', 'nearest']


def analyze_single_run(train_csv_path, test_csv_path, output_dir):
    """
    åˆ†æå•ä¸ªå®éªŒè¿è¡Œï¼Œç”Ÿæˆè®­ç»ƒæ›²çº¿å›¾å¹¶æå–å…³é”®æŒ‡æ ‡ã€‚
    """
    try:
        df_train = pd.read_csv(train_csv_path)
        df_test = pd.read_csv(test_csv_path)

        if df_train.empty or df_test.empty:
            return None  # å¦‚æœæ–‡ä»¶ä¸ºç©ºåˆ™è·³è¿‡

        # --- 1. æå–å…³é”®æ€§èƒ½æŒ‡æ ‡ ---
        best_test_acc = df_test['accuracy'].max()
        best_epoch = df_test['accuracy'].idxmax() + 1
        final_test_acc = df_test['accuracy'].iloc[-1]
        final_train_acc = df_train['accuracy'].iloc[-1]

        stats = {
            "best_test_accuracy": best_test_acc,
            "best_epoch": best_epoch,
            "final_test_accuracy": final_test_acc,
            "final_train_accuracy": final_train_acc,
            "overfitting_gap": final_train_acc - final_test_acc
        }

        # --- 2. ç»˜åˆ¶å¹¶ä¿å­˜è®­ç»ƒè¿‡ç¨‹å›¾ ---
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

        # ä»è·¯å¾„ä¸­è§£æå‡ºå‚æ•°ç”¨äºæ ‡é¢˜
        parts = output_dir.split(os.sep)
        model, method, rate = parts[-1], parts[-2], parts[-3]
        fig.suptitle(f'Training Dynamics for {model} ({rate}, {method})', fontsize=16)

        # å‡†ç¡®ç‡å­å›¾
        ax1.plot(df_test['epoch'], df_test['accuracy'], 'o-', label=f'Test Acc (Best: {best_test_acc:.4f})',
                 color='royalblue')
        ax1.plot(df_train['epoch'], df_train['accuracy'], '--', label=f'Train Acc (Final: {final_train_acc:.4f})',
                 color='cornflowerblue', alpha=0.8)
        ax1.axvline(x=best_epoch, color='r', linestyle='--', label=f'Best Epoch: {best_epoch}')
        ax1.set_title('Accuracy vs. Epoch')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Accuracy')
        ax1.legend()
        ax1.grid(True, linestyle='--', alpha=0.6)

        # æŸå¤±å­å›¾
        ax2.plot(df_test['epoch'], df_test['loss'], 'o-', label='Test Loss', color='darkorange')
        ax2.plot(df_train['epoch'], df_train['loss'], '--', label='Train Loss', color='sandybrown', alpha=0.8)
        ax2.set_title('Loss vs. Epoch')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Loss')
        ax2.legend()
        ax2.grid(True, linestyle='--', alpha=0.6)

        plt.tight_layout(rect=[0, 0.03, 1, 0.95])

        # å°†å›¾ç‰‡ä¿å­˜åœ¨å¯¹åº”å‚æ•°çš„æ–‡ä»¶å¤¹å†…
        plot_path = os.path.join(output_dir, "training_curves.png")
        plt.savefig(plot_path)
        plt.close()

        return stats

    except Exception as e:
        print(f"  - é”™è¯¯: å¤„ç† {test_csv_path} æ—¶å‡ºé”™: {e}")
        return None


def main(base_path, dataset_name, exp_name):
    """ä¸»å‡½æ•°ï¼Œéå†æ‰€æœ‰å®éªŒå¹¶è°ƒç”¨åˆ†æå‡½æ•°ã€‚"""

    # å®šä¹‰è¾“å…¥å’Œè¾“å‡ºçš„æ ¹ç›®å½•
    metrics_base_path = os.path.join(base_path, dataset_name, "Metrics", exp_name)
    analysis_output_path = os.path.join(base_path, dataset_name, "Analysis", exp_name + "_individual")
    os.makedirs(analysis_output_path, exist_ok=True)

    print(f"ğŸ“Š åˆ†æç»“æœå°†ä¿å­˜è‡³: {os.path.abspath(analysis_output_path)}")

    all_stats = []

    # éå†æ‰€æœ‰å®éªŒç»„åˆ
    for rate in SAMPLE_RATES:
        for method in INTERPOLATION_METHODS:
            for model in MODELS:
                print(f"\n--- æ­£åœ¨åˆ†æ: Rate={rate}, Method={method}, Model={model} ---")

                # æ„å»ºè·¯å¾„
                current_exp_dir = os.path.join(metrics_base_path, f"rate_{rate}", f"interp_{method}", model)
                train_csv = os.path.join(current_exp_dir, "train_metrics.csv")
                test_csv = os.path.join(current_exp_dir, "test_metrics.csv")

                if not (os.path.exists(train_csv) and os.path.exists(test_csv)):
                    print("  - æ‰¾ä¸åˆ°æŒ‡æ ‡æ–‡ä»¶ï¼Œè·³è¿‡ã€‚")
                    continue

                # ä¸ºè¯¥æ¬¡å®éªŒåˆ›å»ºç‹¬ç«‹çš„åˆ†æè¾“å‡ºç›®å½•
                individual_output_dir = os.path.join(analysis_output_path, f"rate_{rate}", f"interp_{method}", model)
                os.makedirs(individual_output_dir, exist_ok=True)

                # åˆ†æå¹¶ç»˜å›¾ï¼Œè·å–ç»Ÿè®¡æ•°æ®
                stats = analyze_single_run(train_csv, test_csv, individual_output_dir)

                if stats:
                    print("  - åˆ†æå®Œæˆï¼Œè®­ç»ƒæ›²çº¿å›¾å·²ä¿å­˜ã€‚")
                    # å°†å‚æ•°ä¿¡æ¯åŠ å…¥åˆ°ç»Ÿè®¡å­—å…¸ä¸­
                    stats['model'] = model
                    stats['sample_rate'] = rate
                    stats['interpolation'] = method
                    all_stats.append(stats)

    # --- ç”Ÿæˆæ€»çš„ç»Ÿè®¡æŠ¥å‘Š ---
    if all_stats:
        print("\n--- æ­£åœ¨ç”Ÿæˆæ‰€æœ‰å®éªŒçš„ç»Ÿè®¡æ€»è§ˆCSVæ–‡ä»¶ ---")
        summary_df = pd.DataFrame(all_stats)

        # è°ƒæ•´åˆ—é¡ºåºï¼Œä½¿å…¶æ›´æ˜“è¯»
        cols_order = ['model', 'sample_rate', 'interpolation', 'best_test_accuracy', 'best_epoch',
                      'final_test_accuracy', 'final_train_accuracy', 'overfitting_gap']
        summary_df = summary_df[cols_order]

        summary_path = os.path.join(analysis_output_path, "all_runs_statistics.csv")
        summary_df.to_csv(summary_path, index=False)
        print(f"âœ… ç»Ÿè®¡æ€»è§ˆå·²ä¿å­˜è‡³: {summary_path}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Analyze individual runs of a grid search experiment.")
    parser.add_argument('--dataset_root', type=str, default='../../datasets/sense-fi/',
                        help='Path to the datasets root directory.')
    parser.add_argument('--dataset', type=str, default='NTU-Fi_HAR', help='Dataset name to analyze.')
    parser.add_argument('--exp_name', type=str, required=True, help='The main grid search experiment name to analyze.')

    args = parser.parse_args()

    main(args.dataset_root, args.dataset, args.exp_name)