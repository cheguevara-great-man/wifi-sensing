#ç”¨äºå¯¹æ¯”ä¸¤æ¬¡å®éªŒçš„æ€§èƒ½
#ç”¨æ³•
#python compare_experiments.py --exp1 "amp_500hz_baseline_20250724_2241" --exp2 "energy_500hz_baseline_20250725_1614"

'''
import os
import pandas as pd
import matplotlib.pyplot as plt
import argparse
import numpy as np

# --- é…ç½®åŒº ---
MODELS_TO_COMPARE = [
    'MLP', 'LeNet', 'ResNet18', 'ResNet50', 'ResNet101', 'RNN',
    'GRU', 'LSTM', 'BiLSTM', 'CNN+GRU', 'ViT'
]


def analyze_and_compare(base_path, dataset_name, exp1_name, exp2_name):
    """ä¸»åˆ†æå‡½æ•°ï¼Œè´Ÿè´£åŠ è½½æ•°æ®ã€ç”Ÿæˆè¡¨æ ¼å’Œç»˜å›¾ã€‚"""

    # åˆ›å»ºç”¨äºå­˜æ”¾å¯¹æ¯”å›¾çš„ç›®å½•
    plot_output_dir = os.path.join(base_path, dataset_name, "ComparisonResults", f"{exp1_name}_vs_{exp2_name}")
    os.makedirs(plot_output_dir, exist_ok=True)
    print(f"ğŸ“ˆ å¯¹æ¯”å›¾å°†ä¿å­˜è‡³: {os.path.abspath(plot_output_dir)}")

    comparison_results = []

    for model_name in MODELS_TO_COMPARE:
        print(f"\n--- æ­£åœ¨å¤„ç†æ¨¡å‹: {model_name} ---")

        try:
            # --- 1. åŠ è½½æ•°æ® ---
            # æ„å»ºä¸¤ä¸ªå®éªŒçš„æŒ‡æ ‡æ–‡ä»¶è·¯å¾„
            path1 = os.path.join(base_path, dataset_name, "Metrics", exp1_name, model_name)
            path2 = os.path.join(base_path, dataset_name, "Metrics", exp2_name, model_name)

            df_train1 = pd.read_csv(os.path.join(path1, "train_metrics.csv"))
            df_test1 = pd.read_csv(os.path.join(path1, "test_metrics.csv"))

            df_train2 = pd.read_csv(os.path.join(path2, "train_metrics.csv"))
            df_test2 = pd.read_csv(os.path.join(path2, "test_metrics.csv"))

            # --- 2. æå–å…³é”®æ€§èƒ½æŒ‡æ ‡ ---
            # å®éªŒ1 (æŒ¯å¹…)
            best_acc1 = df_test1['accuracy'].max()
            best_epoch1 = df_test1['accuracy'].idxmax() + 1
            final_test_acc1 = df_test1['accuracy'].iloc[-1]
            final_train_acc1 = df_train1['accuracy'].iloc[-1]

            # å®éªŒ2 (èƒ½é‡)
            best_acc2 = df_test2['accuracy'].max()
            best_epoch2 = df_test2['accuracy'].idxmax() + 1
            final_test_acc2 = df_test2['accuracy'].iloc[-1]
            final_train_acc2 = df_train2['accuracy'].iloc[-1]

            comparison_results.append({
                "Model": model_name,
                "Amp Best Acc": best_acc1,
                "Amp Best Epoch": best_epoch1,
                "Energy Best Acc": best_acc2,
                "Energy Best Epoch": best_epoch2,
            })

            # --- 3. ç»˜åˆ¶å¹¶ä¿å­˜å¯¹æ¯”å›¾ ---
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
            fig.suptitle(f'Comparison for Model: {model_name}', fontsize=16)

            # å‡†ç¡®ç‡å­å›¾
            ax1.plot(df_test1['epoch'], df_test1['accuracy'], 'o-', label=f'Amp Test (Best: {best_acc1:.4f})',
                     color='royalblue')
            ax1.plot(df_test2['epoch'], df_test2['accuracy'], 's-', label=f'Energy Test (Best: {best_acc2:.4f})',
                     color='darkorange')
            ax1.plot(df_train1['epoch'], df_train1['accuracy'], '--', label='Amp Train', color='cornflowerblue',
                     alpha=0.7)
            ax1.plot(df_train2['epoch'], df_train2['accuracy'], '--', label='Energy Train', color='sandybrown',
                     alpha=0.7)
            ax1.set_title('Accuracy vs. Epoch')
            ax1.set_xlabel('Epoch')
            ax1.set_ylabel('Accuracy')
            ax1.legend()
            ax1.grid(True, linestyle='--', alpha=0.6)

            # æŸå¤±å­å›¾
            ax2.plot(df_test1['epoch'], df_test1['loss'], 'o-', label='Amp Test Loss', color='royalblue')
            ax2.plot(df_test2['epoch'], df_test2['loss'], 's-', label='Energy Test Loss', color='darkorange')
            ax2.set_title('Loss vs. Epoch')
            ax2.set_xlabel('Epoch')
            ax2.set_ylabel('Loss')
            ax2.legend()
            ax2.grid(True, linestyle='--', alpha=0.6)

            plt.tight_layout(rect=[0, 0.03, 1, 0.95])
            plot_path = os.path.join(plot_output_dir, f"comparison_{model_name}.png")
            plt.savefig(plot_path)
            plt.close()
            print(f"  - å¯¹æ¯”å›¾å·²ä¿å­˜: {plot_path}")

        except FileNotFoundError:
            print(f"  - è­¦å‘Š: æ‰¾ä¸åˆ°æ¨¡å‹ '{model_name}' åœ¨æŸä¸ªå®éªŒä¸­çš„æŒ‡æ ‡æ–‡ä»¶ï¼Œè·³è¿‡æ­¤æ¨¡å‹ã€‚")
            continue

    return comparison_results


def print_summary_table(results, exp1_name, exp2_name):
    """æ‰“å°æ ¼å¼åŒ–çš„æ€»ç»“è¡¨æ ¼ã€‚"""
    if not results:
        print("\næœªèƒ½ç”Ÿæˆä»»ä½•å¯¹æ¯”ç»“æœã€‚")
        return

    print("\n\n" + "=" * 80)
    print(" " * 25 + "å®éªŒæ€§èƒ½å¯¹æ¯”æ€»ç»“")
    print("=" * 80)
    print(f"å®éªŒ 1 (Amp): {exp1_name}")
    print(f"å®éªŒ 2 (Energy): {exp2_name}")
    print("-" * 80)
    # æ‰“å°è¡¨å¤´
    print(f"{'Model':<12} | {'Amp Best Acc':<15} | {'Energy Best Acc':<17} | {'Winner':<8} | {'Improvement':<12}")
    print("-" * 80)

    for res in results:
        winner = "Energy" if res['Energy Best Acc'] > res['Amp Best Acc'] else "Amp"
        if abs(res['Energy Best Acc'] - res['Amp Best Acc']) < 0.0001:
            winner = "Tie"

        improvement = abs(res['Energy Best Acc'] - res['Amp Best Acc'])

        # å†³å®šèµ¢å®¶é¢œè‰² (ANSI escape codes)
        GREEN = '\033[92m'
        RED = '\033[91m'
        ENDC = '\033[0m'

        if winner == "Energy":
            energy_str = f"{GREEN}{res['Energy Best Acc']:.4f}{ENDC}"
            amp_str = f"{res['Amp Best Acc']:.4f}"
            winner_str = f"{GREEN}{winner}{ENDC}"
            improvement_str = f"+{improvement:.2%}"
        elif winner == "Amp":
            energy_str = f"{res['Energy Best Acc']:.4f}"
            amp_str = f"{GREEN}{res['Amp Best Acc']:.4f}{ENDC}"
            winner_str = f"{RED}{winner}{ENDC}"
            improvement_str = f"-{improvement:.2%}"
        else:
            energy_str = f"{res['Energy Best Acc']:.4f}"
            amp_str = f"{res['Amp Best Acc']:.4f}"
            winner_str = "Tie"
            improvement_str = "N/A"

        print(f"{res['Model']:<12} | {amp_str:<24} | {energy_str:<26} | {winner_str:<18} | {improvement_str:<12}")
    print("-" * 80)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Compare two training experiments.")
    parser.add_argument('--dataset_root', type=str, default='../../datasets/sense-fi/',
                        help='Path to the datasets root directory.')
    parser.add_argument('--dataset', type=str, default='NTU-Fi_HAR', help='Dataset name to analyze.')
    parser.add_argument('--exp1', type=str, required=True, help='Name of the first experiment (e.g., amplitude run).')
    parser.add_argument('--exp2', type=str, required=True, help='Name of the second experiment (e.g., energy run).')

    args = parser.parse_args()

    results = analyze_and_compare(args.dataset_root, args.dataset, args.exp1, args.exp2)
    print_summary_table(results, args.exp1, args.exp2)'''


#ä¸Šé¢æ˜¯ä¸ºæ¯ä¸ªæ¨¡å‹å•ç‹¬ç”Ÿæˆå¯¹æ¯”å›¾ï¼Œä¸‹é¢æ˜¯å°†æ‰€æœ‰æ¨¡å‹çš„æ€§èƒ½å¯¹æ¯”æ±‡æ€»åˆ°ä¸€å¼ å›¾ä¸­
'''å°†ç°æœ‰çš„ compare_experiments.py è„šæœ¬è¿›è¡Œä¿®æ”¹ï¼Œä¸å†ä¸ºæ¯ä¸ªæ¨¡å‹å•ç‹¬ç”Ÿæˆå¯¹æ¯”å›¾ï¼Œè€Œæ˜¯å°†æ‰€æœ‰æ¨¡å‹çš„æ€§èƒ½å¯¹æ¯”æ±‡æ€»åˆ°ä¸€å¼ å›¾ä¸­ï¼Œä»è€Œå¯¹ä¸¤æ¬¡å®éªŒçš„æ€»ä½“æ€§èƒ½å·®å¼‚æœ‰ä¸€ä¸ªå®è§‚ã€ç›´è§‚çš„è®¤è¯†ã€‚
æˆ‘ä»¬å°†å†æ¬¡ä½¿ç”¨å“‘é“ƒå›¾ï¼ˆDumbbell Plotï¼‰ï¼Œå› ä¸ºå®ƒéå¸¸é€‚åˆå±•ç¤ºâ€œä¹‹å‰ vs. ä¹‹åâ€æˆ–è€…â€œA vs. Bâ€çš„å¯¹æ¯”ã€‚
ä¿®æ”¹æ€è·¯
    ç§»é™¤å•æ¨¡å‹ç»˜å›¾: æˆ‘ä»¬å°†ä» analyze_and_compare å‡½æ•°çš„ for å¾ªç¯ä¸­ç§»é™¤æ‰€æœ‰ä¸ matplotlib ç›¸å…³çš„ç»˜å›¾ä»£ç ã€‚ç°åœ¨ï¼Œè¿™ä¸ªå¾ªç¯åªè´Ÿè´£æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„æ€§èƒ½æ•°æ®ã€‚
    åˆ›å»ºæ–°çš„ç»˜å›¾å‡½æ•°: æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªæ–°çš„å‡½æ•°ï¼Œä¾‹å¦‚ plot_overall_comparisonã€‚
    ä¼ å…¥æ±‡æ€»æ•°æ®: è¿™ä¸ªæ–°å‡½æ•°å°†æ¥æ”¶ analyze_and_compare å‡½æ•°è¿”å›çš„ã€åŒ…å«äº†æ‰€æœ‰æ¨¡å‹å¯¹æ¯”ç»“æœçš„ DataFrameã€‚
ç»˜åˆ¶å“‘é“ƒå›¾:
    Yè½´æ˜¯æ¨¡å‹åç§°ã€‚
    å¯¹äºæ¯ä¸ªæ¨¡å‹ï¼Œéƒ½ä¼šæœ‰ä¸€æ¡æ°´å¹³çº¿ã€‚
    çº¿çš„ä¸€ä¸ªç«¯ç‚¹ä»£è¡¨å®ƒåœ¨å®éªŒ1ï¼ˆæŒ¯å¹…ï¼‰ä¸­çš„æœ€ä½³å‡†ç¡®ç‡ã€‚
    çº¿çš„å¦ä¸€ä¸ªç«¯ç‚¹ä»£è¡¨å®ƒåœ¨å®éªŒ2ï¼ˆèƒ½é‡ï¼‰ä¸­çš„æœ€ä½³å‡†ç¡®ç‡ã€‚
    æˆ‘ä»¬å°†ç”¨é¢œè‰²æ¥åŒºåˆ†å“ªä¸ªå®éªŒæ›´å¥½ã€‚ä¾‹å¦‚ï¼Œå¦‚æœèƒ½é‡å®éªŒçš„æ€§èƒ½æ›´é«˜ï¼Œé‚£ä¹ˆè¿æ¥çº¿å¯ä»¥æ˜¯ç»¿è‰²çš„ï¼›å¦‚æœæŒ¯å¹…å®éªŒæ›´å¥½ï¼Œè¿æ¥çº¿å¯ä»¥æ˜¯çº¢è‰²çš„ã€‚'''
import os
import pandas as pd
import matplotlib.pyplot as plt
import argparse
import numpy as np

# --- é…ç½®åŒº ---
MODELS_TO_COMPARE = [
    'MLP', 'LeNet', 'ResNet18', 'ResNet50', 'ResNet101', 'RNN',
    'GRU', 'LSTM', 'BiLSTM', 'CNN+GRU', 'ViT'
]


def analyze_and_compare(base_path, dataset_name, exp1_name, exp2_name):
    """
    ä¸»åˆ†æå‡½æ•°ï¼Œä¸ºæ¯ä¸ªæ¨¡å‹åŠ è½½æ•°æ®å¹¶æå–ã€æœ€ä½³ã€‘å’Œã€æœ€ç»ˆã€‘ä¸¤ç§æ€§èƒ½æŒ‡æ ‡ã€‚
    """
    comparison_results = []

    label1 = exp1_name.split('_')[0] if '_' in exp1_name else exp1_name
    label2 = exp2_name.split('_')[0] if '_' in exp2_name else exp2_name

    for model_name in MODELS_TO_COMPARE:
        print(f"\n--- æ­£åœ¨å¤„ç†æ¨¡å‹: {model_name} ---")
        try:
            path1 = os.path.join(base_path, dataset_name, "Metrics", exp1_name, model_name)
            path2 = os.path.join(base_path, dataset_name, "Metrics", exp2_name, model_name)

            df_test1 = pd.read_csv(os.path.join(path1, "test_metrics.csv"))
            df_test2 = pd.read_csv(os.path.join(path2, "test_metrics.csv"))

            # æå–å®éªŒ1çš„ä¸¤ç§æŒ‡æ ‡
            best_acc1 = df_test1['accuracy'].max()
            final_acc1 = df_test1['accuracy'].iloc[-1]

            # æå–å®éªŒ2çš„ä¸¤ç§æŒ‡æ ‡
            best_acc2 = df_test2['accuracy'].max()
            final_acc2 = df_test2['accuracy'].iloc[-1]

            comparison_results.append({
                "Model": model_name,
                f"{label1}_Best_Acc": best_acc1,
                f"{label2}_Best_Acc": best_acc2,
                f"{label1}_Final_Acc": final_acc1,
                f"{label2}_Final_Acc": final_acc2,
            })
            print(f"  - Best Acc: {label1}={best_acc1:.4f}, {label2}={best_acc2:.4f}")
            print(f"  - Final Acc: {label1}={final_acc1:.4f}, {label2}={final_acc2:.4f}")

        except FileNotFoundError:
            print(f"  - è­¦å‘Š: æ‰¾ä¸åˆ°æ¨¡å‹ '{model_name}' åœ¨æŸä¸ªå®éªŒä¸­çš„æŒ‡æ ‡æ–‡ä»¶ï¼Œè·³è¿‡æ­¤æ¨¡å‹ã€‚")
            continue

    return pd.DataFrame(comparison_results)


def plot_overall_comparison(summary_df, metric_type, exp1_name, exp2_name, output_dir):
    """
    ç»˜åˆ¶ä¸€å¼ åŒ…å«æ‰€æœ‰æ¨¡å‹å¯¹æ¯”çš„å“‘é“ƒå›¾ã€‚
    Args:
        summary_df (pd.DataFrame): åŒ…å«æ‰€æœ‰æ€§èƒ½æ•°æ®çš„DataFrameã€‚
        metric_type (str): 'Best' æˆ– 'Final'ï¼Œå†³å®šä½¿ç”¨å“ªä¸¤åˆ—æ•°æ®è¿›è¡Œç»˜å›¾ã€‚
        exp1_name, exp2_name: å®éªŒåç§°ã€‚
        output_dir: å›¾ç‰‡ä¿å­˜ç›®å½•ã€‚
    """
    if summary_df.empty:
        return

    label1_prefix = exp1_name.split('_')[0] if '_' in exp1_name else exp1_name
    label2_prefix = exp2_name.split('_')[0] if '_' in exp2_name else exp2_name

    col1 = f"{label1_prefix}_{metric_type}_Acc"
    col2 = f"{label2_prefix}_{metric_type}_Acc"

    # æŒ‰ç¬¬äºŒä¸ªå®éªŒçš„æŒ‡å®šæ€§èƒ½æŒ‡æ ‡è¿›è¡Œæ’åº
    df_sorted = summary_df.sort_values(by=col2, ascending=True)

    fig, ax = plt.subplots(figsize=(12, 10))

    for i, model_name in enumerate(df_sorted['Model']):
        row = df_sorted[df_sorted['Model'] == model_name].iloc[0]
        acc1 = row[col1]
        acc2 = row[col2]
        color = 'forestgreen' if acc2 > acc1 else 'firebrick' if acc1 > acc2 else 'grey'
        ax.plot([acc1, acc2], [i, i], marker='', alpha=0.7, color=color, linewidth=2)

    ax.scatter(df_sorted[col1], range(len(df_sorted)), color='royalblue', s=80, label=f"{label1_prefix} Acc", zorder=3)
    ax.scatter(df_sorted[col2], range(len(df_sorted)), color='darkorange', s=80, label=f"{label2_prefix} Acc", zorder=3)

    ax.set_yticks(range(len(df_sorted)))
    ax.set_yticklabels(df_sorted['Model'])

    ax.set_title(f'Overall Performance Comparison ({metric_type} Accuracy)', fontsize=16)
    ax.set_xlabel(f'{metric_type} Test Accuracy', fontsize=12)
    ax.set_ylabel('Model', fontsize=12)

    ax.legend()
    ax.grid(axis='x', linestyle='--', alpha=0.7)

    plt.tight_layout()
    plot_path = os.path.join(output_dir, f"overall_comparison_{metric_type.lower()}_acc.png")
    plt.savefig(plot_path)
    plt.close()
    print(f"\nğŸ“ˆ {metric_type} æ€§èƒ½å¯¹æ¯”å›¾å·²ä¿å­˜è‡³: {plot_path}")


# print_summary_table å‡½æ•°å¯ä»¥ä¿æŒåŸæ ·ï¼Œå› ä¸ºå®ƒåªå…³æ³¨Best Acc
def print_summary_table(results_df, exp1_name, exp2_name):
    # (è¿™ä¸ªå‡½æ•°ä¸éœ€è¦ä¿®æ”¹ï¼Œä½†æˆ‘ä»¬éœ€è¦ä»DataFrameä¸­æå–æ­£ç¡®çš„åˆ—)
    label1_prefix = exp1_name.split('_')[0] if '_' in exp1_name else exp1_name
    label2_prefix = exp2_name.split('_')[0] if '_' in exp2_name else exp2_name

    # ä¸ºäº†é€‚é…æ—§å‡½æ•°ï¼Œæˆ‘ä»¬ä»å¤§DataFrameä¸­æå–å®ƒéœ€è¦çš„åˆ—
    table_df = results_df[['Model', f'{label1_prefix}_Best_Acc', f'{label2_prefix}_Best_Acc']].copy()
    table_df.rename(columns={
        f'{label1_prefix}_Best_Acc': 'Amp Best Acc',
        f'{label2_prefix}_Best_Acc': 'Energy Best Acc'
    }, inplace=True)

    results_list = table_df.to_dict('records')
    # (æ—§çš„æ‰“å°é€»è¾‘)
    # ...


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Compare two training experiments.")
    parser.add_argument('--dataset_root', type=str, default='../../datasets/sense-fi/',
                        help='Path to the datasets root directory.')
    parser.add_argument('--dataset', type=str, default='NTU-Fi_HAR', help='Dataset name to analyze.')
    parser.add_argument('--exp1', type=str, required=True, help='Name of the first experiment (e.g., amplitude).')
    parser.add_argument('--exp2', type=str, required=True, help='Name of the second experiment (e.g., energy).')

    args = parser.parse_args()

    output_dir = os.path.join(args.dataset_root, args.dataset, "ComparisonResults", f"{args.exp1}_vs_{args.exp2}")
    os.makedirs(output_dir, exist_ok=True)

    # 1. æ”¶é›†æ‰€æœ‰æ•°æ® (åŒ…æ‹¬bestå’Œfinal) - (è¿™éƒ¨åˆ†ä¸å˜)
    results_df = analyze_and_compare(args.dataset_root, args.dataset, args.exp1, args.exp2)

    # 2. ç»˜åˆ¶ã€æœ€ä½³æ€§èƒ½ã€‘å¯¹æ¯”å›¾ - (è¿™éƒ¨åˆ†ä¸å˜)
    plot_overall_comparison(results_df, 'Best', args.exp1, args.exp2, output_dir)

    # 3. ç»˜åˆ¶ã€æœ€ç»ˆæ€§èƒ½ã€‘å¯¹æ¯”å›¾ - (è¿™éƒ¨åˆ†ä¸å˜)
    plot_overall_comparison(results_df, 'Final', args.exp1, args.exp2, output_dir)

    # ==================== æ ¸å¿ƒä¿®æ”¹ï¼šæ–°çš„æ€»ç»“è¡¨æ ¼é€»è¾‘ ====================
    if not results_df.empty:
        # ä»åˆ—åä¸­åŠ¨æ€æå–æ ‡ç­¾
        label1_prefix = args.exp1.split('_')[0] if '_' in args.exp1 else args.exp1
        label2_prefix = args.exp2.split('_')[0] if '_' in args.exp2 else args.exp2

        col_best1 = f"{label1_prefix}_Best_Acc"
        col_best2 = f"{label2_prefix}_Best_Acc"
        col_final1 = f"{label1_prefix}_Final_Acc"
        col_final2 = f"{label2_prefix}_Final_Acc"

        # è®¡ç®—å·®å¼‚
        results_df['Best Acc Change'] = results_df[col_best2] - results_df[col_best1]
        results_df['Final Acc Change'] = results_df[col_final2] - results_df[col_final1]

        # æŒ‰æœ€ä½³æ€§èƒ½å·®å¼‚è¿›è¡Œæ’åºï¼Œçœ‹å“ªä¸ªæ¨¡å‹æå‡æœ€å¤§
        results_df_sorted = results_df.sort_values(by="Best Acc Change", ascending=False)

        # å‡†å¤‡æ‰“å°
        print("\n\n" + "=" * 85)
        print(" " * 15 + "å®éªŒæ€§èƒ½å·®å¼‚å¯¹æ¯”æ€»ç»“ (Energy vs. Amp)")
        print("=" * 85)
        print(f"åŸºå‡†å®éªŒ (Exp 1): {args.exp1}")
        print(f"å¯¹æ¯”å®éªŒ (Exp 2): {args.exp2}")
        print("  - æ­£å€¼è¡¨ç¤º Exp 2 æ€§èƒ½æ›´å¥½")
        print("  - è´Ÿå€¼è¡¨ç¤º Exp 1 æ€§èƒ½æ›´å¥½")
        print("-" * 85)
        print(f"{'Model':<12} | {'Best Acc Change':<20} | {'Final Acc Change':<20} | {col_best2:<20}")
        print("-" * 85)

        # ANSI é¢œè‰²ä»£ç 
        GREEN = '\033[92m'
        RED = '\033[91m'
        ENDC = '\033[0m'

        for _, row in results_df_sorted.iterrows():
            # æ ¼å¼åŒ–æœ€ä½³å‡†ç¡®ç‡å˜åŒ–
            best_change = row['Best Acc Change']
            if best_change > 0:
                best_change_str = f"{GREEN}+{best_change:.2%}{ENDC}"
            else:
                best_change_str = f"{RED}{best_change:.2%}{ENDC}"

            # æ ¼å¼åŒ–æœ€ç»ˆå‡†ç¡®ç‡å˜åŒ–
            final_change = row['Final Acc Change']
            if final_change > 0:
                final_change_str = f"{GREEN}+{final_change:.2%}{ENDC}"
            else:
                final_change_str = f"{RED}{final_change:.2%}{ENDC}"

            # è·å–å®éªŒ2çš„æœ€ä½³å‡†ç¡®ç‡ä½œä¸ºå‚è€ƒ
            best_acc_exp2_str = f"{row[col_best2]:.4f}"

            print(f"{row['Model']:<12} | {best_change_str:<29} | {final_change_str:<29} | {best_acc_exp2_str:<20}")

        print("-" * 85)

        # ä¹Ÿå¯ä»¥é€‰æ‹©ä¿å­˜è¿™ä¸ªå·®å¼‚DataFrameåˆ°CSV
        diff_summary_path = os.path.join(output_dir, "performance_difference_summary.csv")
        results_df[['Model', 'Best Acc Change', 'Final Acc Change']].to_csv(diff_summary_path, index=False)
        print(f"âœ… æ€§èƒ½å·®å¼‚æ€»ç»“å·²ä¿å­˜è‡³: {diff_summary_path}")

    # ======================================================================