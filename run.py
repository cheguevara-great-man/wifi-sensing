import os  # å¼•å…¥ os æ¨¡å—
# ==================== è§£å†³ num_workers å’Œ numpy çš„å†²çª ====================
#ä¸ºäº†è®¾ç½®è¿›ç¨‹ä¸ºå•çº¿ç¨‹ï¼Œå‡å°‘cpuå ç”¨
# æ˜ç¡®æ§åˆ¶ OpenBLAS çš„çº¿ç¨‹æ•°ï¼Œè¿™æ˜¯ä½ å½“å‰NumPyçš„ä¸»è¦åç«¯
os.environ['OPENBLAS_NUM_THREADS'] = '1'
# OpenBLAS å†…éƒ¨ä½¿ç”¨ OpenMPï¼Œæ‰€ä»¥è¿™ä¸ªä¹Ÿå¾ˆé‡è¦
os.environ['OMP_NUM_THREADS'] = '1'
# ç”±äºä½ çš„numpyæ²¡æœ‰é“¾æ¥MKLï¼Œè¿™ä¸¤ä¸ªå˜é‡å¯ä»¥ä¸è®¾ï¼Œä½†è®¾äº†ä¹Ÿæ— å®³ï¼Œå¯ä»¥ä¿ç•™ä»¥é˜²ä¸‡ä¸€æœªæ¥æ›´æ”¹ç¯å¢ƒ
os.environ['MKL_NUM_THREADS'] = '1'
# è¿™ä¸ªé€šå¸¸æ˜¯macOSç‰¹æœ‰çš„ï¼ŒLinuxæœåŠ¡å™¨åŸºæœ¬ç”¨ä¸åˆ°ï¼Œä½†ä¿ç•™æ— å®³
os.environ['VECLIB_MAXIMUM_THREADS'] = '1'
# å¦‚æœä½ ç¡®å®šä¸ä½¿ç”¨BLISï¼Œè¿™ä¸ªå¯ä»¥ä¸è®¾ï¼Œä¿ç•™ä¹Ÿæ— å®³
os.environ['BLIS_NUM_THREADS'] = '1'
os.environ['NUMEXPR_NUM_THREADS'] = '1' # æœ‰æ—¶ä¹Ÿéœ€è¦è¿™ä¸ª
# =========================================================================
import numpy as np
import torch
#ä¸ºäº†è®¾ç½®è¿›ç¨‹ä¸ºå•çº¿ç¨‹ï¼Œå‡å°‘cpuå ç”¨
torch.set_num_threads(1)
import torch.nn as nn
import argparse
from util import load_data_n_model
import time
import csv # 1. å¼•å…¥ csv æ¨¡å—
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.cuda.amp import autocast  # è®°å¾—åŠ ä¸Šè¿™ä¸ª import
from skimage.metrics import structural_similarity as ssim_func

# BGI masks path (edit as needed)
BGI_MASK_PT = "/home/cxy/data/code/datasets/sense-fi/Widar_digit/mask_10_90Hz_random/synthetic_masks_bgi_0p2_rate25.pt"


def is_dist():
    return dist.is_available() and dist.is_initialized()

def get_rank():
    return dist.get_rank() if is_dist() else 0

def is_main():
    return get_rank() == 0



# train_one_epoch å’Œ test_one_epoch å‡½æ•°ä¸ä¸Šä¸€ä¸ªå›ç­”ä¸­çš„ç‰ˆæœ¬ç›¸åŒ
# è¿™é‡Œä¸ºäº†å®Œæ•´æ€§å†æ¬¡åŒ…å«å®ƒä»¬

'''def train_one_epoch(model, tensor_loader, criterion, device, optimizer):
    model.train()
    epoch_loss = 0
    epoch_accuracy = 0
    num_samples = 0

    for data in tensor_loader:
        inputs, labels = data
        inputs = inputs.to(device, dtype=torch.float32)
        labels = labels.to(device, dtype=torch.long)
        #labels = labels.type(torch.LongTensor)

        optimizer.zero_grad()
        outputs = model(inputs)
        #outputs = outputs.to(device)
        #outputs = outputs.type(torch.FloatTensor)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item() * inputs.size(0)
        predict_y = torch.argmax(outputs, dim=1)
        epoch_accuracy += (predict_y == labels).sum().item()
        num_samples += labels.size(0)

    epoch_loss = epoch_loss / num_samples
    epoch_accuracy = epoch_accuracy / num_samples
    return epoch_loss, epoch_accuracy'''
use_amp = True  # æ§åˆ¶æ˜¯å¦å¯ç”¨ AMPï¼ˆé»˜è®¤å¯ç”¨ï¼‰

def train_one_epoch(
    model, tensor_loader, criterion, device, optimizer,
    is_rec: int = 0, criterion_rec=None, alpha: float = 0.5,
    lam_miss=2.0,beta=0.1,log_parts=False,
    grad_check=False           # æ˜¯å¦æ£€æŸ¥æ¢¯åº¦/å‚æ•°æ˜¯å¦åœ¨æ›´æ–°ï¼ˆdebug ç”¨ï¼‰
):
    model.train()
    epoch_loss = 0.0
    epoch_correct = 0
    num_samples = 0
    # --- [æ–°å¢] loss åˆ†é‡ç»Ÿè®¡ ---
    sum_ce = 0.0
    sum_miss_term = 0.0
    sum_known_term = 0.0
    sum_miss_ratio = 0.0
    sum_scale = 0.0
    sum_mse_all_equiv = 0.0
    part_cnt = 0


    first_param_before = None
    if grad_check:
        # ç”¨æ¥éªŒè¯ optimizer çœŸçš„åœ¨æ›´æ–°å‚æ•°
        first_param_before = next(model.parameters()).detach().clone()

    for batch in tensor_loader:
        # ---- 1) æ¬åˆ° deviceï¼ˆä¿æŒ dtype æ­£ç¡®ï¼‰
        optimizer.zero_grad(set_to_none=True)
        if int(is_rec) == 0:
            inputs, labels = batch
            inputs = inputs.to(device, dtype=torch.float32, non_blocking=True)
            labels = labels.to(device, dtype=torch.long, non_blocking=True)
            '''outputs = model(inputs)
            loss = criterion(outputs, labels)'''
            # ä½¿ç”¨ autocast æ¥å¯ç”¨ bf16
            with torch.autocast(device_type="cuda", dtype=torch.bfloat16, enabled=use_amp):
                outputs = model(inputs)
            loss = criterion(outputs, labels)
        else:
            inputs, mask, labels, inputs_gt = batch
            inputs = inputs.to(device, dtype=torch.float32, non_blocking=True)
            mask = mask.to(device, dtype=torch.float32, non_blocking=True)
            labels = labels.to(device, dtype=torch.long, non_blocking=True)
            inputs_gt = inputs_gt.to(device, dtype=torch.float32, non_blocking=True)
            # ä½¿ç”¨ autocast æ¥å¯ç”¨ bf16
            with torch.autocast(device_type="cuda", dtype=torch.bfloat16, enabled=use_amp):
                outputs, x_recon = model(inputs, mask)
                # loss = criterion(outputs, labels) + float(alpha) * criterion_rec(x_recon, inputs_gt)
                # å°† loss è®¡ç®—æ”¾åœ¨ fp32
            x_recon_fp32 = x_recon.float()
            inputs_gt_fp32 = inputs_gt.float()
            m = mask.to(dtype=x_recon_fp32.dtype).clamp(0.0, 1.0)
            miss = 1.0 - m
            diff = (x_recon_fp32 - inputs_gt_fp32)
            mse_miss = (diff.mul(miss)).pow(2).sum() / (miss.sum() + 1e-8)
            mse_known = (diff.mul(m)).pow(2).sum() / (m.sum() + 1e-8)

            # miss_ratio = Nmiss / Nallï¼ˆè¿™æ˜¯ä½ è¯´çš„â€œå æ¯”éšé‡‡æ ·ç‡å˜â€çš„å…³é”®ï¼‰
            Nall = float(m.numel())
            miss_ratio = miss.sum() / (Nall + 1e-8)
            known_ratio = 1.0 - miss_ratio
            ce = criterion(outputs.float(), labels)
            loss = ce + lam_miss *  (miss_ratio * mse_miss)  + beta * (known_ratio * mse_known)
            #loss = ce + lam_miss *  (mse_miss)  + beta * (mse_known)

            if log_parts:
                # miss_ratio = Nmiss / Nall
                miss_ratio = (miss.sum() / (m.numel() + 1e-8)).detach()
                scale = ((m.numel()) / (miss.sum() + 1e-8)).detach()  # Nall/Nmiss
                mse_all_equiv = (mse_miss * miss_ratio).detach()      # â‰ˆ old MSE_all (knownè¯¯å·®â‰ˆ0æ—¶)

                sum_ce += ce.detach().float().item()
                sum_miss_term += (lam_miss * mse_miss).detach().float().item()
                sum_known_term += (beta * mse_known).detach().float().item()
                sum_miss_ratio += miss_ratio.float().item()
                sum_scale += scale.float().item()
                sum_mse_all_equiv += mse_all_equiv.float().item()
                part_cnt += 1

        loss.backward()
        optimizer.step()

        # ---- 4) ç»Ÿè®¡ epoch æŒ‡æ ‡
        bs = inputs.size(0)
        epoch_loss += loss.item() * bs
        pred = outputs.argmax(dim=1)
        epoch_correct += (pred == labels).sum().item()
        num_samples += bs


    if num_samples == 0:
        return 0.0, 0.0
    if is_dist():
        t = torch.tensor([epoch_loss, epoch_correct, num_samples],
                         device=device, dtype=torch.float64)
        dist.all_reduce(t, op=dist.ReduceOp.SUM)
        epoch_loss, epoch_correct, num_samples = t.tolist()
    epoch_loss = epoch_loss / num_samples
    epoch_accuracy = epoch_correct / num_samples

    if grad_check and first_param_before is not None:
        with torch.no_grad():
            first_param_after = next(model.parameters()).detach()
            delta = (first_param_after - first_param_before).abs().mean().item()
        if is_main():print(f"[grad_check] first_param abs-mean delta = {delta:.6e}")
    # --- [æ–°å¢] DDP èšåˆ + æ‰“å° ---
    if log_parts and part_cnt > 0:
        if is_dist():
            t = torch.tensor(
                [sum_ce, sum_miss_term, sum_known_term, sum_miss_ratio, sum_scale, sum_mse_all_equiv, float(part_cnt)],
                device=device, dtype=torch.float64
            )
            dist.all_reduce(t, op=dist.ReduceOp.SUM)
            sum_ce, sum_miss_term, sum_known_term, sum_miss_ratio, sum_scale, sum_mse_all_equiv, part_cnt = t.tolist()

        if is_main():
            denom = max(1.0, part_cnt)
            ce_m = sum_ce / denom
            miss_m = sum_miss_term / denom
            known_m = sum_known_term / denom
            mr = sum_miss_ratio / denom
            sc = sum_scale / denom
            mse_all_eq = sum_mse_all_equiv / denom
            # old 0.5*MSE_all ç­‰æ•ˆé¡¹
            old_like = 0.5 * mse_all_eq
            print(
                f"[loss_parts] miss_ratio={mr:.4f}  Nall/Nmiss={sc:.2f}  "
                f"CE={ce_m:.4f}  lam*mse_miss={miss_m:.4f}  beta*mse_known={known_m:.4f}  "
                f"mse_all_equiv={mse_all_eq:.6f}  old_like(0.5*MSE_all)={old_like:.6f}"
            )

    return epoch_loss, epoch_accuracy


def test_one_epoch(model, tensor_loader, criterion, device,
                   is_rec: int = 0, criterion_rec=None, alpha: float = 0.5,lam_miss=2.0,beta=0.1,):
    model.eval()
    total_loss, total_correct, num_samples = 0.0, 0, 0

    with torch.no_grad():
        for batch in tensor_loader:
            if int(is_rec) == 0:
                inputs, labels = batch
                inputs = inputs.to(device, dtype=torch.float32, non_blocking=True)
                labels = labels.to(device, dtype=torch.long, non_blocking=True)
                # ä½¿ç”¨ autocast å¯ç”¨ bf16
                with torch.autocast(device_type="cuda", dtype=torch.bfloat16, enabled=use_amp):
                    outputs = model(inputs)
                loss = criterion(outputs, labels)

            else:
                inputs, mask, labels, inputs_gt = batch
                inputs = inputs.to(device, dtype=torch.float32, non_blocking=True)
                mask = mask.to(device, dtype=torch.float32, non_blocking=True)
                labels = labels.to(device, dtype=torch.long, non_blocking=True)
                inputs_gt = inputs_gt.to(device, dtype=torch.float32, non_blocking=True)
                with torch.autocast(device_type="cuda", dtype=torch.bfloat16, enabled=use_amp):
                    outputs, x_recon = model(inputs, mask)
                    # loss = criterion(outputs, labels) + float(alpha) * criterion_rec(x_recon, inputs_gt)
                # å°† loss è®¡ç®—æ”¾åœ¨ fp32
                x_recon_fp32 = x_recon.float()
                inputs_gt_fp32 = inputs_gt.float()
                m = mask.to(dtype=x_recon_fp32.dtype).clamp(0.0, 1.0)
                miss = 1.0 - m
                diff = (x_recon_fp32 - inputs_gt_fp32)
                mse_miss = (diff.mul(miss)).pow(2).sum() / (miss.sum() + 1e-8)
                mse_known = (diff.mul(m)).pow(2).sum() / (m.sum() + 1e-8)
                ce = criterion(outputs, labels)
                Nall = float(m.numel())
                miss_ratio = miss.sum() / (Nall + 1e-8)
                known_ratio = 1.0 - miss_ratio
                loss = ce + lam_miss * (miss_ratio * mse_miss) + beta * (known_ratio * mse_known)
                #loss = ce + lam_miss * mse_miss + beta * mse_known
            bs = labels.size(0)
            total_loss += loss.item() * bs
            total_correct += (outputs.argmax(dim=1) == labels).sum().item()
            num_samples += bs
        if num_samples == 0:
            return 0.0, 0.0
        if is_dist():
            t = torch.tensor([total_loss, float(total_correct), float(num_samples)],
                             device=device, dtype=torch.float64)
            dist.all_reduce(t, op=dist.ReduceOp.SUM)
            total_loss, total_correct, num_samples = t.tolist()
        return total_loss / num_samples, total_correct / num_samples


def _spectrogram_mean(x_tf, n_fft=256, hop=128, win=256):
    # x_tf: (B, T, F) -> average over F, then STFT over T
    x_mean = x_tf.mean(dim=-1)  # (B, T)
    window = torch.hann_window(win, device=x_mean.device, dtype=x_mean.dtype)
    spec = torch.stft(x_mean, n_fft=n_fft, hop_length=hop, win_length=win,
                      window=window, return_complex=True)
    spec_mag = spec.abs()  # (B, Fbins, Frames)
    return spec_mag


def _calculate_ssim_standard(img1, img2):
    """
    Standard SSIM with min-max normalization to [0,1].
    """
    img1 = img1.astype(np.float64)
    img2 = img2.astype(np.float64)
    min_val = min(img1.min(), img2.min())
    max_val = max(img1.max(), img2.max())
    range_val = max_val - min_val + 1e-8
    img1 = (img1 - min_val) / range_val
    img2 = (img2 - min_val) / range_val
    return float(ssim_func(img1, img2, data_range=1.0))


def _pcc_global(x, y, eps=1e-8):
    # x, y: 1D numpy arrays
    x = x.astype(np.float64)
    y = y.astype(np.float64)
    vx = x - x.mean()
    vy = y - y.mean()
    denom = (np.sqrt((vx * vx).sum()) * np.sqrt((vy * vy).sum())) + eps
    return float((vx * vy).sum() / denom)


def test_one_epoch_with_metrics(model, tensor_loader, criterion, device,
                                is_rec: int = 0, criterion_rec=None, alpha: float = 0.5,
                                lam_miss=2.0, beta=0.1):
    model.eval()
    total_loss, total_correct, num_samples = 0.0, 0, 0
    sum_ssim = 0.0
    sum_pcc = 0.0
    sum_log_nmse = 0.0
    metric_cnt = 0

    with torch.no_grad():
        for batch in tensor_loader:
            if int(is_rec) == 0:
                inputs, labels = batch
                inputs = inputs.to(device, dtype=torch.float32, non_blocking=True)
                labels = labels.to(device, dtype=torch.long, non_blocking=True)
                with torch.autocast(device_type="cuda", dtype=torch.bfloat16, enabled=use_amp):
                    outputs = model(inputs)
                loss = criterion(outputs, labels)
            else:
                inputs, mask, labels, inputs_gt = batch
                inputs = inputs.to(device, dtype=torch.float32, non_blocking=True)
                mask = mask.to(device, dtype=torch.float32, non_blocking=True)
                labels = labels.to(device, dtype=torch.long, non_blocking=True)
                inputs_gt = inputs_gt.to(device, dtype=torch.float32, non_blocking=True)
                with torch.autocast(device_type="cuda", dtype=torch.bfloat16, enabled=use_amp):
                    outputs, x_recon = model(inputs, mask)
                x_recon_fp32 = x_recon.float()
                inputs_gt_fp32 = inputs_gt.float()
                m = mask.to(dtype=x_recon_fp32.dtype).clamp(0.0, 1.0)
                miss = 1.0 - m
                diff = (x_recon_fp32 - inputs_gt_fp32)
                mse_miss = (diff.mul(miss)).pow(2).sum() / (miss.sum() + 1e-8)
                mse_known = (diff.mul(m)).pow(2).sum() / (m.sum() + 1e-8)
                ce = criterion(outputs, labels)
                Nall = float(m.numel())
                miss_ratio = miss.sum() / (Nall + 1e-8)
                known_ratio = 1.0 - miss_ratio
                loss = ce + lam_miss * (miss_ratio * mse_miss) + beta * (known_ratio * mse_known)

                # metrics: SSIM over spectrograms, PCC over waveform, log NMSE on missing points
                # x_recon_fp32 / inputs_gt_fp32: (B,1,T,F)
                x_rec_tf = x_recon_fp32.squeeze(1)  # (B,T,F)
                x_gt_tf = inputs_gt_fp32.squeeze(1)  # (B,T,F)
                spec_rec = _spectrogram_mean(x_rec_tf)
                spec_gt = _spectrogram_mean(x_gt_tf)
                spec_rec_np = spec_rec.detach().cpu().numpy()
                spec_gt_np = spec_gt.detach().cpu().numpy()
                x_rec_np = x_rec_tf.detach().cpu().numpy().reshape(x_rec_tf.size(0), -1)
                x_gt_np = x_gt_tf.detach().cpu().numpy().reshape(x_gt_tf.size(0), -1)

                # per-sample metrics
                for i in range(x_rec_tf.size(0)):
                    sum_ssim += _calculate_ssim_standard(spec_gt_np[i], spec_rec_np[i])
                    sum_pcc += _pcc_global(x_gt_np[i], x_rec_np[i])

                # log NMSE on missing points
                num = (diff.mul(miss)).pow(2).flatten(1).sum(1)
                den = (inputs_gt_fp32.mul(miss)).pow(2).flatten(1).sum(1) + 1e-8
                log_nmse = 10.0 * torch.log10(num / den + 1e-8)
                sum_log_nmse += log_nmse.sum().item()
                metric_cnt += x_rec_tf.size(0)

            bs = labels.size(0)
            total_loss += loss.item() * bs
            total_correct += (outputs.argmax(dim=1) == labels).sum().item()
            num_samples += bs

        if num_samples == 0:
            return 0.0, 0.0, 0.0, 0.0, 0.0
        if is_dist():
            t = torch.tensor(
                [total_loss, float(total_correct), float(num_samples),
                 sum_ssim, sum_pcc, sum_log_nmse, float(metric_cnt)],
                device=device, dtype=torch.float64
            )
            dist.all_reduce(t, op=dist.ReduceOp.SUM)
            total_loss = t[0].item()
            total_correct = t[1].item()
            num_samples = t[2].item()
            sum_ssim = t[3].item()
            sum_pcc = t[4].item()
            sum_log_nmse = t[5].item()
            metric_cnt = t[6].item()

        acc = total_correct / num_samples
        loss_mean = total_loss / num_samples
        if metric_cnt == 0:
            return loss_mean, acc, 0.0, 0.0, 0.0
        return loss_mean, acc, sum_ssim / metric_cnt, sum_pcc / metric_cnt, sum_log_nmse / metric_cnt


def save_metrics_to_csv(filepath, history):
    """
    å°†æ€§èƒ½å†å²è®°å½•ï¼ˆä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼‰ä¿å­˜åˆ°CSVæ–‡ä»¶ã€‚
    Args:
        filepath (str): CSVæ–‡ä»¶çš„å®Œæ•´è·¯å¾„ã€‚
        history (list of dict): åŒ…å« 'epoch', 'loss', 'accuracy' çš„å­—å…¸åˆ—è¡¨ã€‚
    """
    if not history:
        return

    # ä½¿ç”¨ 'w' æ¨¡å¼æ‰“å¼€æ–‡ä»¶ï¼Œnewline='' æ˜¯csvæ¨¡å—çš„æ¨èåšæ³•
    with open(filepath, 'w', newline='') as f:
        # å®šä¹‰CSVæ–‡ä»¶çš„åˆ—åï¼Œä¸historyå­—å…¸ä¸­çš„é”®å¯¹åº”
        fieldnames = ['epoch', 'loss', 'accuracy']
        writer = csv.DictWriter(f, fieldnames=fieldnames)

        # å†™å…¥è¡¨å¤´
        writer.writeheader()
        # å†™å…¥æ‰€æœ‰è¡Œæ•°æ®
        writer.writerows(history)


def run_per_rate_eval(model, test_loader, criterion, device, args, ddp, criterion_rec=None):
    # per-rate evaluation for trafficlike masks
    if not hasattr(test_loader.dataset, "get_available_rates"):
        return
    rates = test_loader.dataset.get_available_rates()
    if not rates:
        return
    rate_history = []
    if is_main():
        print("Running per-rate evaluation...")
    for r in rates:
        if hasattr(test_loader.dataset, "set_rate_filter"):
            test_loader.dataset.set_rate_filter(r)
        if hasattr(test_loader.dataset, "set_eval_subset"):
            test_loader.dataset.set_eval_subset(len(test_loader.dataset), seed=1000 + int(r))
        r_loss, r_acc, r_ssim, r_pcc, r_log_nmse = test_one_epoch_with_metrics(
            model, test_loader, criterion, device,
            is_rec=args.is_rec, criterion_rec=criterion_rec, alpha=args.rec_alpha,
            lam_miss=args.lam_miss, beta=args.beta
        )
        if is_main():
            print(f"[rate {r}] Loss: {r_loss:.5f}, Accuracy: {r_acc:.4f}, "
                  f"SSIM: {r_ssim:.4f}, PCC: {r_pcc:.4f}, logNMSE_miss: {r_log_nmse:.4f}")
        rate_history.append({
            'rate_hz': int(r),
            'loss': r_loss,
            'accuracy': r_acc,
            'ssim': r_ssim,
            'pcc': r_pcc,
            'log_nmse_miss': r_log_nmse
        })
    if hasattr(test_loader.dataset, "set_rate_filter"):
        test_loader.dataset.set_rate_filter(None)
    if hasattr(test_loader.dataset, "set_eval_subset"):
        test_loader.dataset.set_eval_subset(len(test_loader.dataset), seed=0)
    if is_main():
        rate_path = os.path.join(args.metrics_save_dir, 'test_metrics_by_rate.csv')
        with open(rate_path, 'w', newline='') as f:
            writer = csv.DictWriter(
                f, fieldnames=['rate_hz', 'loss', 'accuracy', 'ssim', 'pcc', 'log_nmse_miss']
            )
            writer.writeheader()
            writer.writerows(rate_history)
        print(f"Saved per-rate metrics to: {rate_path}")


def run_per_bgi_eval(model, test_loader, criterion, device, args, ddp, criterion_rec=None, bgi_mask_pt=None):
    if not bgi_mask_pt:
        return
    if not hasattr(test_loader.dataset, "set_eval_masks"):
        return
    payload = torch.load(bgi_mask_pt, map_location="cpu")
    masks = payload.get("masks", None)
    bgi_bin = payload.get("bgi_bin", None)
    if masks is None or bgi_bin is None:
        raise ValueError("bgi_mask_pt must contain 'masks' and 'bgi_bin'")
    test_loader.dataset.set_eval_masks(masks, bgi_bin=bgi_bin)

    bins = ["0.0-0.2", "0.2-0.4", "0.4-0.6", "0.6-0.8", "0.8-1.0"]
    bin_history = []
    if is_main():
        print("Running per-BGI-bin evaluation...")
    for i, b in enumerate(bins):
        if hasattr(test_loader.dataset, "set_bgi_bin_filter"):
            test_loader.dataset.set_bgi_bin_filter(b)
        if hasattr(test_loader.dataset, "set_eval_subset"):
            test_loader.dataset.set_eval_subset(len(test_loader.dataset), seed=2000 + i)
        r_loss, r_acc, r_ssim, r_pcc, r_log_nmse = test_one_epoch_with_metrics(
            model, test_loader, criterion, device,
            is_rec=args.is_rec, criterion_rec=criterion_rec, alpha=args.rec_alpha,
            lam_miss=args.lam_miss, beta=args.beta
        )
        if is_main():
            print(f"[bin {b}] Loss: {r_loss:.5f}, Accuracy: {r_acc:.4f}, "
                  f"SSIM: {r_ssim:.4f}, PCC: {r_pcc:.4f}, logNMSE_miss: {r_log_nmse:.4f}")
        bin_history.append({
            'bgi_bin': b,
            'loss': r_loss,
            'accuracy': r_acc,
            'ssim': r_ssim,
            'pcc': r_pcc,
            'log_nmse_miss': r_log_nmse
        })

    if hasattr(test_loader.dataset, "set_bgi_bin_filter"):
        test_loader.dataset.set_bgi_bin_filter(None)
    if hasattr(test_loader.dataset, "set_eval_subset"):
        test_loader.dataset.set_eval_subset(len(test_loader.dataset), seed=0)
    if hasattr(test_loader.dataset, "clear_eval_masks"):
        test_loader.dataset.clear_eval_masks()

    if is_main():
        bin_path = os.path.join(args.metrics_save_dir, 'test_metrics_by_bgi.csv')
        with open(bin_path, 'w', newline='') as f:
            writer = csv.DictWriter(
                f, fieldnames=['bgi_bin', 'loss', 'accuracy', 'ssim', 'pcc', 'log_nmse_miss']
            )
            writer.writeheader()
            writer.writerows(bin_history)
        print(f"Saved per-BGI-bin metrics to: {bin_path}")

def main():
    root = '../datasets/sense-fi/'
    if not os.path.isdir(root):
        if is_main():
            print(f"é”™è¯¯: æ•°æ®é›†æ ¹ç›®å½• '{root}' æœªæ‰¾åˆ°ã€‚")
            print("è¯·ç¡®è®¤æ‚¨çš„è„šæœ¬ï¼ˆrun.pyï¼‰æ˜¯å¦åœ¨ 'code/sense-fi/' æ–‡ä»¶å¤¹ä¸‹ï¼Œ")
            print("å¹¶ä¸” 'datasets' æ–‡ä»¶å¤¹åœ¨ 'code/' çš„ä¸Šä¸€çº§ç›®å½•ã€‚")
        return
    parser = argparse.ArgumentParser('WiFi Imaging Benchmark')
    parser.add_argument('--dataset', choices = ['UT_HAR_data','NTU-Fi-HumanID','NTU-Fi_HAR','Widar','Widar_digit_amp','Widar_digit_conj'])
    parser.add_argument('--model', choices = ['MLP','LeNet','ResNet18','ResNet50','ResNet101','RNN','GRU','LSTM','BiLSTM', 'CNN+GRU','ViT'])
    # æ–°å¢çš„å‚æ•°ï¼Œç”¨äºè‡ªå®šä¹‰å®éªŒåç§°ï¼Œå¹¶è®¾ä¸ºå¿…å¡«é¡¹
    #parser.add_argument('--exp_name', required=True, type=str, help='è‡ªå®šä¹‰å®éªŒåç§°ï¼Œå°†ç”¨äºåˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•ã€‚')
    parser.add_argument('--sample_rate', type=float, default=1.0, help='äºŒæ¬¡é™é‡‡æ ·çš„æ¯”ä¾‹ (0.05åˆ°1.0)ï¼Œå¯¹åº”25Hzåˆ°500Hzã€‚é»˜è®¤ä¸º1.0ï¼Œå³ä¸è¿›è¡ŒäºŒæ¬¡é‡‡æ ·ã€‚')
    parser.add_argument('--sample_method', type=str,default='uniform_nearest',choices=['uniform_nearest', 'equidistant', 'gaussian', 'poisson', 'trafficlike'],help='é™é‡‡æ ·æ–¹æ³•ã€‚é»˜è®¤ä¸º "uniform_nearest"ã€‚')
    parser.add_argument('--interpolation', type=str,default='linear',choices=['linear', 'cubic', 'nearest', 'idw', 'rbf','spline','akima'],help='å‡é‡‡æ ·æ—¶ä½¿ç”¨çš„æ’å€¼æ–¹æ³•ã€‚é»˜è®¤ä¸º "linear"ã€‚')
    parser.add_argument('--use_energy_input', type=int, default=1, choices=[0, 1],help='æ˜¯å¦ä½¿ç”¨èƒ½é‡ä¿¡æ¯ (1:æ˜¯, 0:å¦)ã€‚é»˜è®¤ä¸º 1 (æ˜¯)ã€‚')
    parser.add_argument('--use_mask_0', type=int, default=0, choices=[0, 1 , 2],help='æ˜¯å¦ä½¿ç”¨ mask_0 (1:æ˜¯, 0:å¦,2:ä¸maskç›´æ¥returné™é‡‡æ ·åçš„)ã€‚é»˜è®¤ä¸º 0 (å¦)ã€‚')
    parser.add_argument('--traffic_train_pt', type=str, default='/home/cxy/data/code/datasets/sense-fi/Widar_digit/mask_10_90Hz_random/train.pt', help='trafficlike train masks .pt')
    parser.add_argument('--traffic_test_pt', type=str, default='/home/cxy/data/code/datasets/sense-fi/Widar_digit/mask_10_90Hz_random/test.pt', help='trafficlike test masks .pt')
    # æ–°å¢ä¸¤ä¸ªå‚æ•°ï¼Œç”¨äºæ¥æ”¶å®Œæ•´çš„ä¿å­˜ç›®å½•
    parser.add_argument('--model_save_dir', required=True, type=str, help='æ¨¡å‹æ£€æŸ¥ç‚¹çš„å®Œæ•´ä¿å­˜ç›®å½•ã€‚')
    parser.add_argument('--metrics_save_dir', required=True, type=str, help='æ€§èƒ½æŒ‡æ ‡æ–‡ä»¶çš„å®Œæ•´ä¿å­˜ç›®å½•ã€‚')
    parser.add_argument('--is_rec', type=int, default=0, choices=[0, 1], help='1: é‡å»º+åˆ†ç±»ï¼›0: ä»…åˆ†ç±»')
    parser.add_argument('--rec_alpha', type=float, default=0.5, help='é‡å»ºæŸå¤±æƒé‡')
    parser.add_argument('--csdc_blocks', type=int, default=1, help='é‡å»ºblocksæ•°é‡')
    parser.add_argument('--rec_model', type=str, default='csdc', choices=['csdc', 'istanet','mabf','mabf_c','mabf_1d_mix','mabf2','fista', 'fista_fft', 'fista_dct','fista_blockfft'], help='é‡å»ºæ¨¡å‹ç±»å‹')
    parser.add_argument('--global_batch_size', type=int, default=128, help='å…¨å±€batch(æ‰€æœ‰GPUåŠ èµ·æ¥)')
    parser.add_argument('--num_workers_train', type=int, default=6)
    parser.add_argument('--num_workers_test', type=int, default=2)
    parser.add_argument('--lam_miss', type=float, default=1.0, help='é‡å»ºæŸå¤±ä¸­ç¼ºå¤±éƒ¨åˆ†çš„æƒé‡')
    parser.add_argument('--beta', type=float, default=0.0, help='é‡å»ºæŸå¤±ä¸­å·²çŸ¥éƒ¨åˆ†çš„æƒé‡')
    parser.add_argument('--test_only', action='store_true', help='ä»…æµ‹è¯•ï¼Œä¸è®­ç»ƒ')
    parser.add_argument('--ckpt_path', type=str, default=None, help='æµ‹è¯•ç”¨æ¨¡å‹æƒé‡è·¯å¾„(.pth)')
    parser.add_argument('--eval_rate', action='store_true', help='æµ‹è¯•ï¼šæŒ‰é‡‡æ ·ç‡åˆ†æ¡¶')
    parser.add_argument('--eval_bgi', action='store_true', help='æµ‹è¯•ï¼šæŒ‰BGIåˆ†æ¡¶')
    args = parser.parse_args()
    # ---- DDP init (torchrun ä¼šè®¾ç½®è¿™äº›ç¯å¢ƒå˜é‡) ----
    ddp = ("RANK" in os.environ) and ("WORLD_SIZE" in os.environ)
    if ddp:
        dist.init_process_group(backend="nccl", init_method="env://")
        local_rank = int(os.environ["LOCAL_RANK"])
        torch.cuda.set_device(local_rank)
        device = torch.device("cuda", local_rank)
        world_size = dist.get_world_size()
        rank = dist.get_rank()
    else:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        world_size = 1
        rank = 0

    # ---- å…¨å±€128 => æ¯å¡ batch = 128/world_size ----
    if args.global_batch_size % world_size != 0:
        raise ValueError(f"global_batch_size={args.global_batch_size} ä¸èƒ½è¢« world_size={world_size} æ•´é™¤")
    per_gpu_bs = args.global_batch_size // world_size
    train_loader, test_loader, model, train_epoch = load_data_n_model(
        args.dataset, args.model, root,
        args.sample_rate, args.sample_method, args.interpolation,
        args.use_energy_input, args.use_mask_0,
        args.is_rec, args.csdc_blocks, args.rec_model,
        batch_size=per_gpu_bs,
        num_workers_train=args.num_workers_train,
        num_workers_test=args.num_workers_test,
        distributed=ddp, rank=rank, world_size=world_size,
        traffic_train_pt=args.traffic_train_pt,
        traffic_test_pt=args.traffic_test_pt
    )

    #train_loader, test_loader, model, train_epoch = load_data_n_model(args.dataset, args.model, root,args.sample_rate, args.sample_method ,args.interpolation,args.use_energy_input ,args.use_mask_0 ,args.is_rec,args.csdc_blocks)
    criterion = nn.CrossEntropyLoss()
    criterion_rec = nn.MSELoss(reduction='mean') if args.is_rec else None

    #device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    if ddp:
        model = DDP(model, device_ids=[local_rank], output_device=local_rank)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

    # load checkpoint if provided
    if args.ckpt_path:
        state = torch.load(args.ckpt_path, map_location=device)
        if ddp:
            model.module.load_state_dict(state, strict=True)
        else:
            model.load_state_dict(state, strict=True)
        if is_main():
            print(f"Loaded checkpoint: {args.ckpt_path}")

    if args.test_only:
        if not args.ckpt_path:
            raise ValueError("test_only requires --ckpt_path")
        # å›ºå®šä¸€ä»½éªŒè¯æ©ç å­é›†ï¼ˆç”¨äºç¨³å®šå¯¹æ¯”ï¼‰
        if hasattr(test_loader.dataset, "set_rate_filter"):
            test_loader.dataset.set_rate_filter(None)
        if hasattr(test_loader.dataset, "set_eval_subset"):
            test_loader.dataset.set_eval_subset(len(test_loader.dataset), seed=0)
        test_loss, test_acc = test_one_epoch(model, test_loader, criterion, device,
                                             is_rec=args.is_rec, criterion_rec=criterion_rec,
                                             alpha=args.rec_alpha, lam_miss=args.lam_miss, beta=args.beta)
        if is_main():
            print(f"Test/Validation -> Loss: {test_loss:.5f}, Accuracy: {test_acc:.4f}")
        if args.eval_rate:
            run_per_rate_eval(model, test_loader, criterion, device, args, ddp, criterion_rec=criterion_rec)
        if args.eval_bgi:
            run_per_bgi_eval(model, test_loader, criterion, device, args, ddp, criterion_rec=criterion_rec, bgi_mask_pt=BGI_MASK_PT)
        if ddp:
            dist.destroy_process_group()
        return


    # --- ç›®å½•åˆ›å»º ---
    # ç°åœ¨ run.py åªè´Ÿè´£ç¡®ä¿ç›®å½•å­˜åœ¨ï¼Œä¸å†æ„å»ºå®ƒ
    os.makedirs(args.model_save_dir, exist_ok=True)
    os.makedirs(args.metrics_save_dir, exist_ok=True)
    if is_main():
        print(f"âœ… æ¨¡å‹å°†ä¿å­˜è‡³: {os.path.abspath(args.model_save_dir)}")
        print(f"ğŸ“Š æ€§èƒ½æŒ‡æ ‡å°†ä¿å­˜è‡³: {os.path.abspath(args.metrics_save_dir)}")
    # ================================================================
    # 2. è®¡ç®—ä¿å­˜é—´éš”å’Œä¿å­˜ç‚¹
    num_saves = 4
    if train_epoch < num_saves:
        # å¦‚æœæ€»epochæ•°å°äº10ï¼Œåˆ™æ¯ä¸ªepochéƒ½ä¿å­˜
        save_interval = 1
    else:
        save_interval = train_epoch // num_saves

    # åˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰éœ€è¦ä¿å­˜çš„epochç¼–å·çš„é›†åˆï¼Œæ–¹ä¾¿å¿«é€ŸæŸ¥æ‰¾
    save_epochs = set(range(save_interval, train_epoch + 1, save_interval))
    # ç¡®ä¿æœ€åä¸€ä¸ªepochæ€»æ˜¯è¢«ä¿å­˜
    save_epochs.add(train_epoch)

    if is_main():print(f"æ¨¡å‹å°†ä¼šåœ¨ä»¥ä¸‹Epochç»“æŸæ—¶ä¿å­˜: {sorted(list(save_epochs))}")
    # ==========================================================

    # ==================== 4. æ–°å¢ï¼šåˆå§‹åŒ–å†å²è®°å½•åˆ—è¡¨ ====================
    train_history = []
    test_history = []

    # å›ºå®šä¸€ä»½éªŒè¯æ©ç å­é›†ï¼ˆç”¨äºæ—©åœæ›´ç¨³å®šï¼‰
    if hasattr(test_loader.dataset, "set_rate_filter"):
        test_loader.dataset.set_rate_filter(None)
    if hasattr(test_loader.dataset, "set_eval_subset"):
        test_loader.dataset.set_eval_subset(len(test_loader.dataset), seed=0)

    # [æ–°å¢] æ—©åœç›¸å…³çš„å˜é‡
    best_test_acc = 0.0  # è®°å½•å†å²æœ€ä½³å‡†ç¡®ç‡
    patience = 20  # å®¹å¿åº¦ï¼šå¦‚æœ 20 ä¸ª epoch æ²¡æå‡å°±åœæ­¢
    patience_counter = 0  # è®¡æ•°å™¨
    # ===================================================
    # --- è®­ç»ƒä¸»å¾ªç¯ ---
    total_train_start = time.time()
    for epoch in range(1, train_epoch + 1):  # å¾ªç¯ä»1å¼€å§‹ï¼Œæ–¹ä¾¿ä¸epochç¼–å·å¯¹åº”
        if ddp and hasattr(train_loader.sampler, "set_epoch"):
            train_loader.sampler.set_epoch(epoch)
        if hasattr(train_loader.dataset, "set_epoch"):
            train_loader.dataset.set_epoch(epoch)
        if is_main():print(f"--- Epoch {epoch}/{train_epoch} ---")
        epoch_start = time.time()
        log_parts = (epoch <= 3)# å‰3ä¸ªepochæ‰“å°lossåˆ†é‡
        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, device, optimizer,
                                                is_rec=args.is_rec, criterion_rec=criterion_rec, alpha=args.rec_alpha,lam_miss=args.lam_miss,beta=args.beta,log_parts=log_parts)
        if is_main():print(f"Train -> Loss: {train_loss:.5f}, Accuracy: {train_acc:.4f}")

        test_loss, test_acc = test_one_epoch(model, test_loader, criterion, device,
                                             is_rec=args.is_rec, criterion_rec=criterion_rec, alpha=args.rec_alpha, lam_miss=args.lam_miss,beta=args.beta)
        if is_main():print(f"Test/Validation -> Loss: {test_loss:.5f}, Accuracy: {test_acc:.4f}")
        if is_main():
            epoch_time = time.time() - epoch_start
            print(f"Epoch time: {epoch_time:.2f} s")

        # ==================== 5. æ–°å¢ï¼šæ”¶é›†å½“å‰epochçš„æ•°æ® ====================
        train_history.append({'epoch': epoch, 'loss': train_loss, 'accuracy': train_acc})
        test_history.append({'epoch': epoch, 'loss': test_loss, 'accuracy': test_acc})

        # ==================== [æ ¸å¿ƒä¿®æ”¹] æ—©åœä¸æœ€ä½³æ¨¡å‹ä¿å­˜ ====================
        if test_acc > best_test_acc:
            best_test_acc = test_acc
            patience_counter = 0  # é‡ç½®è®¡æ•°å™¨

            # ä¿å­˜æœ€ä½³æ¨¡å‹ (è¦†ç›–å¼ä¿å­˜ï¼Œå§‹ç»ˆåªæœ‰ä¸€ä¸ª best_model.pth)
            # åªæœ‰å½“ Epoch > 10 ä»¥åï¼Œæ‰çœŸæ­£å¼€å§‹æ‰§è¡Œä¿å­˜ç¡¬ç›˜çš„æ“ä½œ
            if epoch > 20:
                best_model_path = os.path.join(args.model_save_dir, 'best_model.pth')
                if is_main():
                    state = model.module.state_dict() if ddp else model.state_dict()
                    torch.save(state, best_model_path)
                    print(f"ğŸŒŸ æ–°çºªå½•ï¼æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (Acc: {best_test_acc:.4f})")
            else:
                if is_main():print(f"ğŸŒŸ æ–°çºªå½• (Acc: {best_test_acc:.4f}) - è®­ç»ƒåˆæœŸæš‚ä¸ä¿å­˜")

        else:
            # åŒæ ·ï¼Œå‰ 10 ä¸ª Epoch ä¹Ÿä¸æ¶ˆè€— patienceï¼ˆå®½å®¹æœŸï¼‰
            if epoch > 20:
                patience_counter += 1
                if is_main():print(f"âš ï¸ æ€§èƒ½æœªæå‡ ({patience_counter}/{patience})")

        # æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢
        if patience_counter >= patience:
            if is_main():
                print(f"\nğŸ›‘ è§¦å‘æ—©åœæœºåˆ¶ï¼æµ‹è¯•é›†å‡†ç¡®ç‡å·²è¿ç»­ {patience} ä¸ª Epoch æœªæå‡ã€‚")
                print(f"   å½“å‰æœ€ä½³å‡†ç¡®ç‡: {best_test_acc:.4f}")
                print(f"   åœ¨ Epoch {epoch} åœæ­¢è®­ç»ƒã€‚")
            break  # è·³å‡º for å¾ªç¯
        # ===================================================================

        # --- æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ä¿å­˜ç‚¹ ---#å‰é¢æœ‰ä¿å­˜æœ€ä½³æ¨¡å‹äº†ï¼Œæ‰€ä»¥è¿™é‡Œä¸å†ä¿å­˜ã€‚
        '''if epoch in save_epochs:
            model_save_path = os.path.join(args.model_save_dir, f'model_epoch_{epoch}.pth')
            print(f"ğŸ’¾ åˆ°è¾¾ä¿å­˜ç‚¹ï¼Œæ­£åœ¨ä¿å­˜æ¨¡å‹åˆ°: {model_save_path}")
            torch.save(model.state_dict(), model_save_path)'''

    total_train_end = time.time()
    if is_main():
        print("\n--- è®­ç»ƒå®Œæˆ ---")
        print(f"â±ï¸ æ€»è®­ç»ƒè€—æ—¶ï¼š{total_train_end - total_train_start:.2f} ç§’")

    # ä½¿ç”¨æ–°çš„ç›®å½•å‚æ•°æ¥æ„å»ºè·¯å¾„
    train_metrics_path = os.path.join(args.metrics_save_dir, 'train_metrics.csv')
    test_metrics_path = os.path.join(args.metrics_save_dir, 'test_metrics.csv')

    if is_main():
        print(f"ğŸ“Š æ­£åœ¨ä¿å­˜è®­ç»ƒå†å²åˆ°: {train_metrics_path}")
        save_metrics_to_csv(train_metrics_path, train_history)

    if is_main():
        print(f"ğŸ“Š æ­£åœ¨ä¿å­˜æµ‹è¯•å†å²åˆ°: {test_metrics_path}")
        save_metrics_to_csv(test_metrics_path, test_history)

    if args.eval_rate:
        run_per_rate_eval(model, test_loader, criterion, device, args, ddp, criterion_rec=criterion_rec)
    if args.eval_bgi:
        run_per_bgi_eval(model, test_loader, criterion, device, args, ddp, criterion_rec=criterion_rec, bgi_mask_pt=BGI_MASK_PT)

    #print(f"ğŸ’¾ æ‰€æœ‰æ£€æŸ¥ç‚¹å·²ä¿å­˜åœ¨ç›®å½•: {args.model_save_dir}")
    if ddp:
        dist.destroy_process_group()

if __name__ == "__main__":
    main()
