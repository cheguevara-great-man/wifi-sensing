import numpy as np
import torch
import torch.nn as nn
import argparse
from util import load_data_n_model
import time
import os  # å¼•å…¥ os æ¨¡å—
import csv # 1. å¼•å…¥ csv æ¨¡å—

# ==================== è§£å†³ num_workers å’Œ numpy çš„å†²çª ====================
# æ˜ç¡®æ§åˆ¶ OpenBLAS çš„çº¿ç¨‹æ•°ï¼Œè¿™æ˜¯ä½ å½“å‰NumPyçš„ä¸»è¦åç«¯
os.environ['OPENBLAS_NUM_THREADS'] = '1'
# OpenBLAS å†…éƒ¨ä½¿ç”¨ OpenMPï¼Œæ‰€ä»¥è¿™ä¸ªä¹Ÿå¾ˆé‡è¦
os.environ['OMP_NUM_THREADS'] = '1'
# ç”±äºä½ çš„numpyæ²¡æœ‰é“¾æ¥MKLï¼Œè¿™ä¸¤ä¸ªå˜é‡å¯ä»¥ä¸è®¾ï¼Œä½†è®¾äº†ä¹Ÿæ— å®³ï¼Œå¯ä»¥ä¿ç•™ä»¥é˜²ä¸‡ä¸€æœªæ¥æ›´æ”¹ç¯å¢ƒ
os.environ['MKL_NUM_THREADS'] = '1'
# è¿™ä¸ªé€šå¸¸æ˜¯macOSç‰¹æœ‰çš„ï¼ŒLinuxæœåŠ¡å™¨åŸºæœ¬ç”¨ä¸åˆ°ï¼Œä½†ä¿ç•™æ— å®³
os.environ['VECLIB_MAXIMUM_THREADS'] = '1'
# å¦‚æœä½ ç¡®å®šä¸ä½¿ç”¨BLISï¼Œè¿™ä¸ªå¯ä»¥ä¸è®¾ï¼Œä¿ç•™ä¹Ÿæ— å®³
os.environ['BLIS_NUM_THREADS'] = '1'
# =========================================================================


# train_one_epoch å’Œ test_one_epoch å‡½æ•°ä¸ä¸Šä¸€ä¸ªå›ç­”ä¸­çš„ç‰ˆæœ¬ç›¸åŒ
# è¿™é‡Œä¸ºäº†å®Œæ•´æ€§å†æ¬¡åŒ…å«å®ƒä»¬

def train_one_epoch(model, tensor_loader, criterion, device, optimizer):
    model.train()
    epoch_loss = 0
    epoch_accuracy = 0
    num_samples = 0

    for data in tensor_loader:
        inputs, labels = data
        inputs = inputs.to(device)
        labels = labels.to(device)
        labels = labels.type(torch.LongTensor)

        optimizer.zero_grad()
        outputs = model(inputs)
        outputs = outputs.to(device)
        outputs = outputs.type(torch.FloatTensor)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item() * inputs.size(0)
        predict_y = torch.argmax(outputs, dim=1)
        epoch_accuracy += (predict_y == labels).sum().item()
        num_samples += labels.size(0)

    epoch_loss = epoch_loss / num_samples
    epoch_accuracy = epoch_accuracy / num_samples
    return epoch_loss, epoch_accuracy


def test_one_epoch(model, tensor_loader, criterion, device):
    model.eval()
    test_acc = 0.0
    test_loss = 0.0
    num_samples = 0

    with torch.no_grad():
        for data in tensor_loader:
            inputs, labels = data
            inputs = inputs.to(device)
            labels = labels.to(device)
            labels = labels.type(torch.LongTensor)

            outputs = model(inputs)
            outputs = outputs.type(torch.FloatTensor)
            outputs.to(device)
            loss = criterion(outputs, labels)

            predict_y = torch.argmax(outputs, dim=1)
            test_acc += (predict_y == labels).sum().item()
            test_loss += loss.item() * inputs.size(0)
            num_samples += labels.size(0)
        test_acc = test_acc / num_samples
        test_loss = test_loss / num_samples
        return test_loss, test_acc


def save_metrics_to_csv(filepath, history):
    """
    å°†æ€§èƒ½å†å²è®°å½•ï¼ˆä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼‰ä¿å­˜åˆ°CSVæ–‡ä»¶ã€‚
    Args:
        filepath (str): CSVæ–‡ä»¶çš„å®Œæ•´è·¯å¾„ã€‚
        history (list of dict): åŒ…å« 'epoch', 'loss', 'accuracy' çš„å­—å…¸åˆ—è¡¨ã€‚
    """
    if not history:
        return

    # ä½¿ç”¨ 'w' æ¨¡å¼æ‰“å¼€æ–‡ä»¶ï¼Œnewline='' æ˜¯csvæ¨¡å—çš„æ¨èåšæ³•
    with open(filepath, 'w', newline='') as f:
        # å®šä¹‰CSVæ–‡ä»¶çš„åˆ—åï¼Œä¸historyå­—å…¸ä¸­çš„é”®å¯¹åº”
        fieldnames = ['epoch', 'loss', 'accuracy']
        writer = csv.DictWriter(f, fieldnames=fieldnames)

        # å†™å…¥è¡¨å¤´
        writer.writeheader()
        # å†™å…¥æ‰€æœ‰è¡Œæ•°æ®
        writer.writerows(history)

def main():
    root = '../datasets/sense-fi/'
    if not os.path.isdir(root):
        print(f"é”™è¯¯: æ•°æ®é›†æ ¹ç›®å½• '{root}' æœªæ‰¾åˆ°ã€‚")
        print("è¯·ç¡®è®¤æ‚¨çš„è„šæœ¬ï¼ˆrun.pyï¼‰æ˜¯å¦åœ¨ 'code/sense-fi/' æ–‡ä»¶å¤¹ä¸‹ï¼Œ")
        print("å¹¶ä¸” 'datasets' æ–‡ä»¶å¤¹åœ¨ 'code/' çš„ä¸Šä¸€çº§ç›®å½•ã€‚")
        return
    parser = argparse.ArgumentParser('WiFi Imaging Benchmark')
    parser.add_argument('--dataset', choices = ['UT_HAR_data','NTU-Fi-HumanID','NTU-Fi_HAR','Widar'])
    parser.add_argument('--model', choices = ['MLP','LeNet','ResNet18','ResNet50','ResNet101','RNN','GRU','LSTM','BiLSTM', 'CNN+GRU','ViT'])
    # æ–°å¢çš„å‚æ•°ï¼Œç”¨äºè‡ªå®šä¹‰å®éªŒåç§°ï¼Œå¹¶è®¾ä¸ºå¿…å¡«é¡¹
    #parser.add_argument('--exp_name', required=True, type=str, help='è‡ªå®šä¹‰å®éªŒåç§°ï¼Œå°†ç”¨äºåˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•ã€‚')
    parser.add_argument('--sample_rate', type=float, default=1.0, help='äºŒæ¬¡é™é‡‡æ ·çš„æ¯”ä¾‹ (0.05åˆ°1.0)ï¼Œå¯¹åº”25Hzåˆ°500Hzã€‚é»˜è®¤ä¸º1.0ï¼Œå³ä¸è¿›è¡ŒäºŒæ¬¡é‡‡æ ·ã€‚')
    parser.add_argument('--interpolation', type=str,default='linear',choices=['linear', 'cubic', 'nearest', 'idw', 'rbf'],help='å‡é‡‡æ ·æ—¶ä½¿ç”¨çš„æ’å€¼æ–¹æ³•ã€‚é»˜è®¤ä¸º "linear"ã€‚')
    # æ–°å¢ä¸¤ä¸ªå‚æ•°ï¼Œç”¨äºæ¥æ”¶å®Œæ•´çš„ä¿å­˜ç›®å½•
    parser.add_argument('--model_save_dir', required=True, type=str, help='æ¨¡å‹æ£€æŸ¥ç‚¹çš„å®Œæ•´ä¿å­˜ç›®å½•ã€‚')
    parser.add_argument('--metrics_save_dir', required=True, type=str, help='æ€§èƒ½æŒ‡æ ‡æ–‡ä»¶çš„å®Œæ•´ä¿å­˜ç›®å½•ã€‚')
    args = parser.parse_args()

    train_loader, test_loader, model, train_epoch = load_data_n_model(args.dataset, args.model, root,args.sample_rate,args.interpolation)
    criterion = nn.CrossEntropyLoss()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)


    # --- ç›®å½•åˆ›å»º ---
    # ç°åœ¨ run.py åªè´Ÿè´£ç¡®ä¿ç›®å½•å­˜åœ¨ï¼Œä¸å†æ„å»ºå®ƒ
    os.makedirs(args.model_save_dir, exist_ok=True)
    os.makedirs(args.metrics_save_dir, exist_ok=True)
    print(f"âœ… æ¨¡å‹å°†ä¿å­˜è‡³: {os.path.abspath(args.model_save_dir)}")
    print(f"ğŸ“Š æ€§èƒ½æŒ‡æ ‡å°†ä¿å­˜è‡³: {os.path.abspath(args.metrics_save_dir)}")
    # ================================================================
    # 2. è®¡ç®—ä¿å­˜é—´éš”å’Œä¿å­˜ç‚¹
    num_saves = 10
    if train_epoch < num_saves:
        # å¦‚æœæ€»epochæ•°å°äº10ï¼Œåˆ™æ¯ä¸ªepochéƒ½ä¿å­˜
        save_interval = 1
    else:
        save_interval = train_epoch // num_saves

    # åˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰éœ€è¦ä¿å­˜çš„epochç¼–å·çš„é›†åˆï¼Œæ–¹ä¾¿å¿«é€ŸæŸ¥æ‰¾
    save_epochs = set(range(save_interval, train_epoch + 1, save_interval))
    # ç¡®ä¿æœ€åä¸€ä¸ªepochæ€»æ˜¯è¢«ä¿å­˜
    save_epochs.add(train_epoch)
    print(f"æ¨¡å‹å°†ä¼šåœ¨ä»¥ä¸‹Epochç»“æŸæ—¶ä¿å­˜: {sorted(list(save_epochs))}")
    # ==========================================================

    # ==================== 4. æ–°å¢ï¼šåˆå§‹åŒ–å†å²è®°å½•åˆ—è¡¨ ====================
    train_history = []
    test_history = []

    # --- è®­ç»ƒä¸»å¾ªç¯ ---
    total_train_start = time.time()
    for epoch in range(1, train_epoch + 1):  # å¾ªç¯ä»1å¼€å§‹ï¼Œæ–¹ä¾¿ä¸epochç¼–å·å¯¹åº”
        print(f"--- Epoch {epoch}/{train_epoch} ---")

        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, device, optimizer)
        print(f"Train -> Loss: {train_loss:.5f}, Accuracy: {train_acc:.4f}")

        test_loss, test_acc = test_one_epoch(model, test_loader, criterion, device)
        print(f"Test/Validation -> Loss: {test_loss:.5f}, Accuracy: {test_acc:.4f}")

        # ==================== 5. æ–°å¢ï¼šæ”¶é›†å½“å‰epochçš„æ•°æ® ====================
        train_history.append({'epoch': epoch, 'loss': train_loss, 'accuracy': train_acc})
        test_history.append({'epoch': epoch, 'loss': test_loss, 'accuracy': test_acc})

        # --- æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ä¿å­˜ç‚¹ ---
        '''if epoch in save_epochs:
            model_save_path = os.path.join(args.model_save_dir, f'model_epoch_{epoch}.pth')
            print(f"ğŸ’¾ åˆ°è¾¾ä¿å­˜ç‚¹ï¼Œæ­£åœ¨ä¿å­˜æ¨¡å‹åˆ°: {model_save_path}")
            torch.save(model.state_dict(), model_save_path)'''

    total_train_end = time.time()
    print("\n--- è®­ç»ƒå®Œæˆ ---")
    print(f"â±ï¸ æ€»è®­ç»ƒè€—æ—¶ï¼š{total_train_end - total_train_start:.2f} ç§’")

    # ä½¿ç”¨æ–°çš„ç›®å½•å‚æ•°æ¥æ„å»ºè·¯å¾„
    train_metrics_path = os.path.join(args.metrics_save_dir, 'train_metrics.csv')
    test_metrics_path = os.path.join(args.metrics_save_dir, 'test_metrics.csv')

    print(f"ğŸ“Š æ­£åœ¨ä¿å­˜è®­ç»ƒå†å²åˆ°: {train_metrics_path}")
    save_metrics_to_csv(train_metrics_path, train_history)

    print(f"ğŸ“Š æ­£åœ¨ä¿å­˜æµ‹è¯•å†å²åˆ°: {test_metrics_path}")
    save_metrics_to_csv(test_metrics_path, test_history)

    #print(f"ğŸ’¾ æ‰€æœ‰æ£€æŸ¥ç‚¹å·²ä¿å­˜åœ¨ç›®å½•: {args.model_save_dir}")

if __name__ == "__main__":
    main()